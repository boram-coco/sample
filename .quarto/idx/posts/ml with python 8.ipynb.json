{"title":"지도 학습","markdown":{"yaml":{"title":"지도 학습","author":"김보람","date":"05/11/2023","categories":["지도 학습"]},"headingText":"ref","containsRefs":false,"markdown":"\n\n\n- 선형대수와 통계학으로 배우는 머신러닝 with 파이썬\n\n- [github](https://github.com/bjpublic/MachineLearning)\n\n# k-최근법 이웃 알고리즘\n\n# 선형 회귀 분석\n\n```python\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import LinearRegression \n\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n\n\n# 데이터 불러오기\nraw_boston = datasets.load_boston()\n\n# 피쳐, 타겟 데이터 지정\nX = raw_boston.data\ny = raw_boston.target\n\n# 트레이닝/테스트 데이터 분할\nX_tn, X_te, y_tn, y_te = train_test_split(X,y,random_state=1)\n\n\n# 데이터 표준화\nstd_scale = StandardScaler()\nstd_scale.fit(X_tn)\nX_tn_std = std_scale.transform(X_tn)\nX_te_std  = std_scale.transform(X_te)\n\n# 선형 회귀분석 학습\nclf_lr =  LinearRegression()\nclf_lr.fit(X_tn_std, y_tn)\n\n# 선형 회귀분석 모형 추정 계수 확인\nprint(clf_lr.coef_)\nprint(clf_lr.intercept_)\n\n# 릿지 회귀분석(L2 제약식 적용)\nclf_ridge = Ridge(alpha=1)\nclf_ridge.fit(X_tn_std, y_tn)\n\n# 릿지 회귀분석 모형 추정 계수 확인\nprint(clf_ridge.coef_)\nprint(clf_ridge.intercept_)\n\n# 라쏘 회귀분석(L1 제약식 적용)\nclf_lasso = Lasso(alpha=0.01)\nclf_lasso.fit(X_tn_std, y_tn)\n\n# 라쏘 회귀분석 모형 추정 계수 확인\nprint(clf_lasso.coef_)\nprint(clf_lasso.intercept_)\n\n# 엘라스틱넷\nclf_elastic = ElasticNet(alpha=0.01, l1_ratio=0.01)\nclf_elastic.fit(X_tn_std, y_tn)\n\n# 엘라스틱넷 모형 추정 계수 확인\nprint(clf_elastic.coef_)\nprint(clf_elastic.intercept_)\n\n# 예측\npred_lr = clf_lr.predict(X_te_std)\npred_ridge = clf_ridge.predict(X_te_std)\npred_lasso = clf_lasso.predict(X_te_std)\npred_elastic = clf_elastic.predict(X_te_std)\n\n# 모형 평가-R제곱값\nprint(r2_score(y_te, pred_lr))\nprint(r2_score(y_te, pred_ridge))\nprint(r2_score(y_te, pred_lasso))\nprint(r2_score(y_te, pred_elastic))\n\n# 모형 평가-MSE\nprint(mean_squared_error(y_te, pred_lr))\nprint(mean_squared_error(y_te, pred_ridge))\nprint(mean_squared_error(y_te, pred_lasso))\nprint(mean_squared_error(y_te, pred_elastic))\n```\n\n`-` 회귀분석\n\n$$\\hat w = (X^TX)^{-1}X^Ty$$\n\n`-` 릿지 회귀 분석(L2제약식)\n\n$$\\hat w^{ridge} = (X^TX+ \\lambda I_p)^{-1}X^Ty$$\n\n- $\\lambda$ 계수의 사이즈 조절, 정규식의 크기 조절, 0에 가까울수록 최소 제곱 추정량에 가까워지며 무한대에 가까워질수록 릿지 해는 0에 가까워짐\n\n- 편향(bias)가 존재\n\n`-` 라쏘 회귀 분석(L1제약식)\n\n$$\\hat w^{lasso}=argmin_w \\{(y-Xw)^T(y-Xw)+\\lambda(|w|-t) \\}$$\n\n# 로지스틱 회귀 분석\n\n# 나이브 베이즈(추후 다시)\n\n```python\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\n\n# 데이터 불러오기\nraw_wine = datasets.load_wine()\n\n# 피쳐, 타겟 데이터 지정\nX = raw_wine.data\ny = raw_wine.target\n\n# 트레이닝/테스트 데이터 분할\nX_tn, X_te, y_tn, y_te=train_test_split(X,y,random_state=0)\n\n# 데이터 표준화\nstd_scale = StandardScaler()\nstd_scale.fit(X_tn)\nX_tn_std = std_scale.transform(X_tn)\nX_te_std  = std_scale.transform(X_te)\n\n# 나이브 베이즈 학습\nclf_gnb = GaussianNB()\nclf_gnb.fit(X_tn_std, y_tn)\n\n# 예측\npred_gnb = clf_gnb.predict(X_te_std)\nprint(pred_gnb)\n\n# 리콜\nrecall = recall_score(y_te, pred_gnb, average='macro')\nprint(recall)\n\n# confusion matrix 확인 \nconf_matrix = confusion_matrix(y_te, pred_gnb)\nprint(conf_matrix)\n\n# 분류 레포트 확인\nclass_report = classification_report(y_te, pred_gnb)\nprint(class_report)\n```\n\n# 의사결정나무(추후 다시)\n\n- 테스트 성능 평가는 엔트로피 이용\n\n- 엔트로피는 불순도(노드에 서로 다른 데이터가 얼마나 섞여 있는지) 정도를 측정하며 낮을수록 좋다. \n\n$$Entropy(d) = - \\sum p(x) log P(x)$$ \n\n$$= - \\sum_{i=1}^k p(i|d)log_2(p(i|d))$$\n\n# 서포트 벡터 머신(추후 다시)\n\n# 크로스 밸리데이션(추후 다시)\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"output-file":"ml with python 8.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.335","theme":"cosmo","title-block-banner":true,"title":"지도 학습","author":"김보람","date":"05/11/2023","categories":["지도 학습"]},"extensions":{"book":{"multiFile":true}}}}}