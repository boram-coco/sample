[
  {
    "objectID": "posts/VAE.html",
    "href": "posts/VAE.html",
    "title": "VAE(-ing)",
    "section": "",
    "text": "https://arxiv.org/pdf/1312.6114.pdf"
  },
  {
    "objectID": "posts/VAE.html#오토인코더",
    "href": "posts/VAE.html#오토인코더",
    "title": "VAE(-ing)",
    "section": "오토인코더",
    "text": "오토인코더\n\nEncoder, Decoder 네트워크로 구성된 모델\n학습 데이터-> encoder에 입력값"
  },
  {
    "objectID": "posts/untitled.html",
    "href": "posts/untitled.html",
    "title": "sample",
    "section": "",
    "text": "1+1\n\n2"
  },
  {
    "objectID": "posts/9999.html",
    "href": "posts/9999.html",
    "title": "sample",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport networkx as nx\nimport sklearn\n\n# split \nfrom sklearn.model_selection import train_test_split\n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n# models \nfrom sklearn.ensemble import RandomForestClassifier \n\n# 평가 \nfrom sklearn import metrics \n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x>0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G, {(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부 \n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\ndef down_sample_textbook(df):\n    df = df[df[\"is_fraud\"]==0].sample(frac=0.20, random_state=42).append(df[df[\"is_fraud\"] == 1])\n    df_majority = df[df.is_fraud==0]\n    df_minority = df[df.is_fraud==1]\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef split(Graph,test_size=0.20,random_state=42):\n    edg = list(range(len(Graph.edges))) \n    edg_att = list(nx.get_edge_attributes(Graph, \"label\").values())\n    return train_test_split(edg,edg_att,test_size=test_size,random_state=random_state) \n\ndef embedding(Graph):\n    _edgs = list(Graph.edges)\n    _train_edges, _test_edges, y, yy = split(Graph)\n    _train_graph = Graph.edge_subgraph([_edgs[x] for x in _train_edges]).copy()\n    _train_graph.add_nodes_from(list(set(Graph.nodes) - set(_train_graph.nodes)))\n    _embedded = AverageEmbedder(Node2Vec(_train_graph, weight_key='weight').fit(window=10).wv)\n    X = [_embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in _train_edges]\n    XX = [_embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in _test_edges]\n    return X,XX,y,yy \n\ndef evaluate(lrnr,XX,yy):\n    yyhat = lrnr.predict(XX)\n    df = pd.DataFrame({'pre':[sklearn.metrics.precision_score(yy,yyhat)], \n                  'rec':[sklearn.metrics.recall_score(yy,yyhat)],\n                  'f1':[sklearn.metrics.f1_score(yy,yyhat)]})\n    return df \n\ndef anal(df,n_estimators=10):\n    Graph = build_graph_bipartite(df)\n    X,XX,y,yy = embedding(Graph)\n    lrnr = RandomForestClassifier(n_estimators=n_estimators, random_state=42) \n    lrnr.fit(X,y)\n    return lrnr, XX,yy, evaluate(lrnr,XX,yy)\n\ndef our_sampling1(df):\n    cus_list = set(df.query('is_fraud==1').cc_num.tolist())\n    return df.query(\"cc_num in @ cus_list\")"
  },
  {
    "objectID": "posts/9999.html#read-and-define-data",
    "href": "posts/9999.html#read-and-define-data",
    "title": "sample",
    "section": "read and define data",
    "text": "read and define data\n\ndf = pd.read_csv(\"~/Desktop/fraudTrain.csv\")\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      trans_date_trans_time\n      cc_num\n      merchant\n      category\n      amt\n      first\n      last\n      gender\n      street\n      ...\n      lat\n      long\n      city_pop\n      job\n      dob\n      trans_num\n      unix_time\n      merch_lat\n      merch_long\n      is_fraud\n    \n  \n  \n    \n      669418\n      669418\n      2019-10-12 18:21\n      4.089100e+18\n      fraud_Haley, Jewess and Bechtelar\n      shopping_pos\n      7.53\n      Debra\n      Stark\n      F\n      686 Linda Rest\n      ...\n      32.3836\n      -94.8653\n      24536\n      Multimedia programmer\n      1983-10-14\n      d313353fa30233e5fab5468e852d22fc\n      1350066071\n      32.202008\n      -94.371865\n      0\n    \n    \n      32567\n      32567\n      2019-01-20 13:06\n      4.247920e+12\n      fraud_Turner LLC\n      travel\n      3.79\n      Judith\n      Moss\n      F\n      46297 Benjamin Plains Suite 703\n      ...\n      39.5370\n      -83.4550\n      22305\n      Television floor manager\n      1939-03-09\n      88c65b4e1585934d578511e627fe3589\n      1327064760\n      39.156673\n      -82.930503\n      0\n    \n    \n      156587\n      156587\n      2019-03-24 18:09\n      4.026220e+12\n      fraud_Klein Group\n      entertainment\n      59.07\n      Debbie\n      Payne\n      F\n      204 Ashley Neck Apt. 169\n      ...\n      41.5224\n      -71.9934\n      4720\n      Broadcast presenter\n      1977-05-18\n      3bd9ede04b5c093143d5e5292940b670\n      1332612553\n      41.657152\n      -72.595751\n      0\n    \n    \n      1020243\n      1020243\n      2020-02-25 15:12\n      4.957920e+12\n      fraud_Monahan-Morar\n      personal_care\n      25.58\n      Alan\n      Parsons\n      M\n      0547 Russell Ford Suite 574\n      ...\n      39.6171\n      -102.4776\n      207\n      Network engineer\n      1955-12-04\n      19e16ee7a01d229e750359098365e321\n      1361805120\n      39.080346\n      -103.213452\n      0\n    \n    \n      116272\n      116272\n      2019-03-06 23:19\n      4.178100e+15\n      fraud_Kozey-Kuhlman\n      personal_care\n      84.96\n      Jill\n      Flores\n      F\n      639 Cruz Islands\n      ...\n      41.9488\n      -86.4913\n      3104\n      Horticulturist, commercial\n      1981-03-29\n      a0c8641ca1f5d6e243ed5a2246e66176\n      1331075954\n      42.502065\n      -86.732664\n      0\n    \n  \n\n5 rows × 23 columns\n\n\n\n\n# df_downsampled = down_sample_textbook(df)"
  },
  {
    "objectID": "posts/9999.html#embedding",
    "href": "posts/9999.html#embedding",
    "title": "sample",
    "section": "embedding",
    "text": "embedding\n\n#G_down = build_graph_bipartite(df_downsampled)\n\n\n# X,XX,y,yy = embedding(G_down)"
  },
  {
    "objectID": "posts/9999.html#learn",
    "href": "posts/9999.html#learn",
    "title": "sample",
    "section": "learn",
    "text": "learn\n\n# lrnr = RandomForestClassifier(n_estimators=10, random_state=42) \n# lrnr.fit(X,y)"
  },
  {
    "objectID": "posts/9999.html#evaluate",
    "href": "posts/9999.html#evaluate",
    "title": "sample",
    "section": "evaluate",
    "text": "evaluate\n\n# evaluate(lrnr,XX,yy)"
  },
  {
    "objectID": "posts/9999.html#read-and-define-data-1",
    "href": "posts/9999.html#read-and-define-data-1",
    "title": "sample",
    "section": "read and define data",
    "text": "read and define data\n\ndf = pd.read_csv(\"~/Desktop/fraudTrain.csv\")\nlrnr2, _,_,_ = anal(down_sample_textbook(our_sampling1(df)),n_estimators=100)\n\n\n\n\nGenerating walks (CPU: 1):   0%|          | 0/10 [00:00<?, ?it/s]\n\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:03<00:00,  3.01it/s]"
  },
  {
    "objectID": "posts/2023_06_13_fin.html",
    "href": "posts/2023_06_13_fin.html",
    "title": "15wk: 기말고사",
    "section": "",
    "text": "(1) 자연수집합 \\(\\mathbb{N}\\)와 짝수의집합 \\(\\{2,4,6,\\dots\\}\\)의 cardinality는 동일하다.\nT. 자연수집합의 카디널리티는 알레프 널, 짝수의 집합도 알레프 널\n(2) 카운팅메져 \\(\\#\\)는 가측공간 \\((\\mathbb{N},2^{\\mathbb{N}})\\)에서 \\(\\sigma\\)-finite measure이다.\nT. \\(A_1=\\{1\\}, A_2=\\{1,2\\}, A_3=\\{1,2,3\\}\\)로 두게 되면 \\(A_1 \\cup \\dots \\cup A_n = \\cup_{i=1}^n A_i = \\mathbb{N}\\)\n\\(m(A_1)=1, m(A_2)=2, \\dots \\forall i \\in \\mathbb{Z}, m(A_i)<\\infty\\)이므로 \\(\\sigma\\)-finite measure이다.\n(3)\n(4)\n(5)\n(6)\n(7)\n(8)\n(9)\n(10)"
  },
  {
    "objectID": "posts/2023_06_13_fin.html#section",
    "href": "posts/2023_06_13_fin.html#section",
    "title": "15wk: 기말고사",
    "section": "(1)",
    "text": "(1)\n(1) \\(\\Omega=\\{1,2,3,4\\}\\) 이라고 하고 \\({\\cal A} = \\{\\emptyset, \\{1\\},\\{2\\},\\{3,4\\},\\Omega\\}\\) 이라고 하자. 함수 \\(\\tilde{P}:{\\cal A} \\to [0,1]\\)를 아래와 같이 정의하자.\n\n\\(\\tilde{P}(\\emptyset) = 0\\)\n\\(\\tilde{P}(\\{1\\}) = 1/4\\)\n\\(\\tilde{P}(\\{2\\}) = 1/2\\)\n\\(\\tilde{P}(\\{3,4\\}) = 1/4\\)\n\\(\\tilde{P}(\\Omega) = 1\\)\n\n\\({\\cal A}\\)에서 \\(\\tilde{P}\\)와 일치하는 확률메져 \\(P\\)가 가측공간 \\((\\Omega,\\sigma({\\cal A}))\\) 에서 유일하게 존재하는가?\nY\n(풀이) 유일하게 존재한다.\n\n\\(P\\) is probability measure on \\((\\Omega,\\sigma({\\cal A}))\\)\n\\(\\forall A \\in {\\cal A}: ~ P(A) = \\tilde{P}(A)\\)\n\n\\(\\sigma({\\cal A}) = \\big\\{\\emptyset, \\{1\\}, \\{2\\}, \\{1,2\\}, \\{3,4\\}, \\{1,3,4\\}, \\{2,3,4\\}, \\Omega \\big\\}\\)\n\n\n\n\n\\(P\\)\n\\(\\tilde{P}\\)\n\n\n\n\n\\(\\emptyset\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(\\{1\\}\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\\(\\{2\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\{3,4\\}\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{4}\\)\n\n\n\\(\\Omega\\)\n\\(1\\)\n\\(1\\)\n\n\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\n\n\\(\\{1,2\\}\\)\n\\(\\frac{3}{4}\\)\nNone\n\n\n\\(\\{1,3,4\\}\\)\n\\(\\frac{1}{2}\\)\nNone\n\n\n\\(\\{2,3,4\\}\\)\n\\(\\frac{3}{4}\\)\nNone\n\n\n\n위와 같이 풀 수 있꼬 또는,\n<[카라데오도리 확장정리]>\n\\(\\cal A\\)에서 확률비슷한 적당한 함수 \\(\\tilde P:\\cal A \\to [0,1]\\)를 잘 정의하면 이 함수는 \\((\\Omega, \\sigma(\\cal A))\\)에서의 확률측도 \\(P\\)로 업그레이드 할 수 있고, 그 결과는 유일하다.\n(풀이2)\n\\(\\cal A\\)는 파이시스템이다. 6주차..\n- Thm: \\({\\cal A}\\)가 \\(\\Omega\\)에 대한 semiring이라고 하자. 함수 \\(\\tilde{m}: {\\cal A} \\to [0,\\infty]\\)가\n\n\\(\\tilde{m}(\\emptyset)=0\\)\n\\(\\tilde{m}(\\uplus_{i=1}^{n} B_i)=\\sum_{i=1}^{n}\\tilde{m}(B_i)\\) additivity 조건\n\\(\\tilde{m}(\\cup_{i=1}^{\\infty} A_i) \\leq \\sum_{i=1}^{\\infty}\\tilde{m}(A_i)\\) \\(\\sigma\\)-subadd 조건\n\n\n\\(\\sigma\\)-additivity조건은 위의 2,3번으로 쪼개졌다고 생각하자.\n\n\n\\(\\exists A_1,A_2 \\dots \\in {\\cal A}\\) with \\(\\tilde{m}(A_i)<\\infty\\) such that \\(\\cup_{i=1}^{\\infty}A_i = \\Omega\\)\n\n\n1~3번은\\(\\tilde{m}\\)가 \\({\\cal A}\\)에서 msr 비슷, 4번까지하면 \\(\\tilde{m}\\)가 \\({\\cal A}\\)에서 \\(\\sigma\\)-finte msr 비슷\nadd \\(\\rightarrow\\) subadd, \\(\\sigma\\)-add \\(\\rightarrow\\) \\(\\sigma\\)-subadd\n\n를 만족한다면 \\(\\tilde{m}\\)은 \\((\\Omega,\\sigma({\\cal A})\\)에서의 측도 \\(m\\)으로 업그레이드 가능하며, 이 업그레이드 결과는 유일하다.\n- 조건체크\n\n\\({\\cal A}\\)는 세미알지브라(그러므로 세미링)이다.\n\\({\\cal A}\\)는 전체집합을 포함하고 있으며 \\({\\tilde P}(\\Omega)=1\\)이다. \\(\\Rightarrow\\) 조건 (4)가 만족.\n\\({\\tilde P}\\)는 (1) \\(\\tilde{P}(\\emptyset)=0\\) 이고 (2) add 를 만족하며 (3) \\(\\sigma\\)-subadd 를 만족한다.\n\n\n참고: 이 예제의 경우 \\(|\\Omega|<\\infty\\) 이므로 \\(\\sigma\\)-subadd 는 subadd 와 같은 성질이다. 그리고 add 는 subadd를 imply 하므로 사실상 (2) 만 체크하면 끝난다.1"
  },
  {
    "objectID": "posts/2023_06_13_fin.html#section-1",
    "href": "posts/2023_06_13_fin.html#section-1",
    "title": "15wk: 기말고사",
    "section": "(2)",
    "text": "(2)\n(2) \\(\\Omega=\\{1,2,3,4\\}\\) 이라고 하고 \\({\\cal A} = \\{\\emptyset, \\{1,2\\},\\{2,3\\}, \\Omega\\}\\) 라고 하자. 함수 \\(\\tilde{P}:{\\cal A} \\to [0,1]\\)를 아래와 같이 정의하자.\n\n\\(\\tilde{P}(\\emptyset) = 0\\)\n\\(\\tilde{P}(\\{1,2\\}) = 1/2\\)\n\\(\\tilde{P}(\\{2,3\\}) = 1/2\\)\n\\(\\tilde{P}(\\Omega) = 1\\)\n\n\\({\\cal A}\\)에서 \\(\\tilde{P}\\)와 일치하는 확률메져 \\(P\\)가 가측공간 \\((\\Omega,\\sigma({\\cal A}))\\) 에서 유일하게 존재하는가?\nF\n\\(\\cal A\\)는 파이시스템이 아니다. \\(\\{2\\}\\)가 없기 떄문에..\n\\(\\hat P\\)는 \\(\\cal A\\)에서 확률의 공리 1~3을 만족한다.\n\\(\\sigma(\\cal A)\\)로 확장 가능\n\n\n\n\n\\(P_1\\)\n\\(P_2\\)\n\\(\\tilde{P}\\)\n\n\n\n\n\\(\\emptyset\\)\n\\(0\\)\n\\(0\\)\n\\(0\\)\n\n\n\\(\\{1,2\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\{2,3\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\n\n\n\\(\\Omega\\)\n\\(1\\)\n\\(1\\)\n\\(1\\)\n\n\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\n\n\\(\\{1\\}\\)\n\\(0\\)\n\\(\\frac{1}{2}\\)\nNone\n\n\n\\(\\{2\\}\\)\n\\(\\frac{1}{2}\\)\n\\(0\\)\nNone\n\n\n\\(\\{3\\}\\)\n\\(0\\)\n\\(\\frac{1}{2}\\)\nNone\n\n\n\\(\\{4\\}\\)\n\\(\\frac{1}{2}\\)\n\\(0\\)\nNone\n\n\n\\(\\{1,3\\}\\)\n\\(0\\)\n\\(1\\)\nNone\n\n\n\\(\\{1,4\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\nNone\n\n\n\\(\\{2,4\\}\\)\n\\(1\\)\n\\(0\\)\nNone\n\n\n\\(\\{3,4\\}\\)\n\\(\\frac{1}{2}\\)\n\\(\\frac{1}{2}\\)\nNone\n\n\n\\(\\{2,3,4\\}\\)\n\\(1\\)\n\\(\\frac{1}{2}\\)\nNone\n\n\n\\(\\{1,3,4\\}\\)\n\\(\\frac{1}{2}\\)\n\\(1\\)\nNone\n\n\n\\(\\{1,2,4\\}\\)\n\\(1\\)\n\\(\\frac{1}{2}\\)\nNone\n\n\n\\(\\{1,2,3\\}\\)\n\\(\\frac{1}{2}\\)\n\\(1\\)\nNone\n\n\n\n- 체크: \\(\\tilde{P}\\)는 괜찮게 정의되었다. (1)-(4)가 모두 성립한다. (위의 예제와는 다르게 subadd 역시 성립함!!) 하지만 \\({\\cal A}\\)가 세미링이 아니어서 탈락.\n\n\n\n\n\n\n주의사항\n\n\n\n1. 위의 두 문제는 정답만 쓸 경우 답안으로 인정하지 않음. (1번답: 유일하게 존재함 <– 이러면 0점)\n2. 유일하게 존재하는 (혹은 존재하지 않는) 이유를 설명하고, 그러한 확률측도를 예시로 제시해야함.\n3. 즉 유일하게 존재하는 경우는 아래를 만족하는 \\(P\\)를 제시하고,\n\n\\(P\\) is probability measure on \\((\\Omega,\\sigma({\\cal A}))\\)\n\\(\\forall A \\in {\\cal A}: ~ P(A) = \\tilde{P}(A)\\)\n\n유일하게 존재하지 않는 경우는 아래를 만족하는 서로 다른 2개의 측도 \\(P_1,P_2\\)를 제시해야함.\n\n\\(P_1,P_2\\) are probability measures on \\((\\Omega,\\sigma({\\cal A}))\\)\n\\(\\forall A \\in {\\cal A}: ~ P_1(A) =P_2(A) = \\tilde{P}(A)\\)\n\\(\\exists B \\in \\sigma({\\cal A}):~ P_1(B) \\neq P_2(B)\\)"
  },
  {
    "objectID": "posts/2023_06_13_fin.html#section-2",
    "href": "posts/2023_06_13_fin.html#section-2",
    "title": "15wk: 기말고사",
    "section": "(1)",
    "text": "(1)\n함수 \\(X:\\Omega \\to \\mathbb{R}\\)을 아래와 같이 정의할때\n\\[X(\\omega) = \\begin{cases} 0 & \\omega \\in \\Omega \\cap \\mathbb{Q}^c \\\\ 1 & \\omega \\in \\Omega \\cap \\mathbb{Q} \\end{cases}\\]\n\\(X\\)가 확률변수임을 보여라.\n(확률변수 정의)\n두 개의 잴 수 있는 공간 \\((\\Omega, \\cal F)\\)와 \\((\\mathbb{R},\\cal R)\\)이 있다고 하자. 확률변수 \\(X\\)는 아래를 만족하는 함수 \\(X:\\Omega \\to \\mathbb{R}\\)이다.\n\\(\\forall B \\in {\\cal R}: X^{-1}(B) = \\{w : X(w) \\in B \\} \\in {\\cal F}\\)\n(풀이) 확률변수임을 체크하기 위해서는\n\n\\(B=\\emptyset\\)일 경우: \\(\\{w: X(w) \\in \\emptyset \\} = \\emptyset \\in \\sigma(A)\\)\n\\(B=\\{0\\}\\)일 경우: \\(\\{w: X(w) \\in \\{0\\} \\} = [0,2\\pi) \\cap \\mathbb{Q}^c \\in \\sigma(A)\\)\n\\(B=\\{1\\}\\)일 경우: \\(\\{w: X(w) \\in \\{1\\} \\} = [0,2\\pi) \\cap \\mathbb{Q} \\in \\sigma(A)\\)\n\\(B=\\{0,1\\}\\)일 경우: \\(\\{w: X(w) \\in \\{0,1\\} \\} = [0,2\\pi) \\in \\sigma(A)\\)\n\n(풀이 좀더 나아가서)\n\\(\\forall B \\in \\cal R, X^{-1}(B) \\in \\cal F\\)임을 보이자.\n\\(X^{-1}(B) = \\{w|X(w) \\in B \\}\\)이다.\n\n\\(0 \\notin B and 1 \\notin B\\)이면 \\(X(w)\\)는 함수 정의에 의해 함수 값이 0 또는 1의 값을 가진다.\n\n어떤 w에 대해서도 \\(X(w) \\notin B\\)이다.\n즉, \\(X^{-1}(B) = \\emptyset \\in \\cal F\\)\n\n\\(0 \\in B and 1 \\notin B\\)\n\n그러면 \\(X(w)=0\\)인 경우는 \\(X(w) \\in B\\)이고 \\(X(w)=1\\)인 경우는 \\(X(w) \\notin B\\)이다.\n따라서 \\(X^{-1}(B) = \\{w|x(w)=0\\}\\)이 된다.\n\\(X(w) = 0 \\leftrightarrow w \\in \\Omega \\cap \\mathbb{Q}^c\\)\n\\(X^{-1}(B) = \\Omega \\cap \\mathbb{Q}^c \\in \\cal F\\)\n\n\\(0 \\notin B and 1 \\in B\\)\n\n그러면 \\(X^{-1}(B) = \\Omega \\cap \\mathbb{Q} \\in \\cal F\\)\n\n\\(0 \\in and 1 \\in B\\)\n\n그러면 \\(X^{-1}(B) = \\Omega \\in \\cal F\\)"
  },
  {
    "objectID": "posts/2023_06_13_fin.html#section-3",
    "href": "posts/2023_06_13_fin.html#section-3",
    "title": "15wk: 기말고사",
    "section": "(2)",
    "text": "(2)\n\\(\\mu_X << \\nu\\) 를 만족하는 \\(\\sigma\\)-finite measure \\(\\nu\\) 를 제시하고, \\(\\mu_X\\)의 Radon-Nikodym derivative (w.r.t. \\(\\nu\\))\n\\[f:=\\frac{d\\mu_X}{d\\nu}\\]\n를 제시하라. 단, 여기에서 \\(\\mu_X : P \\circ X^{-1}\\) 이다.\n<풀이>\nRandon-Nikodym정의 : \\(\\forall B \\in \\cal R: \\mu_X(B) = \\int_B f d\\nu\\)\n\n\\(0 \\notin B\\) and \\(1 \\notin B\\)\n\n\\(P \\circ X^{-1}(B) = P(\\emptyset) = \\frac{0}{2\\pi} = 0\\)\n\n\\(0 \\in B\\) and \\(1 \\notin B\\)\n\n\\(P \\circ X^{-1}(B) = P(\\Omega \\cap \\mathbb{Q}^c) = \\frac{\\lambda(\\mathbb{Q}^c)}{2\\pi}=\\frac{2\\pi}{2\\pi}=1\\)\n\n\\(0 \\notin B\\) and \\(1 \\in B\\)\n\n\\(P \\circ X^{-1}(B) = P(\\Omega \\cap \\mathbb{Q}) = \\frac{\\lambda(\\mathbb{Q})}{2\\pi}=\\frac{0}{2\\pi}=0\\)\n\n\\(0 \\in B\\) and \\(1 \\in B\\)\n\n\\(P \\circ X^{-1}(\\Omega)= \\frac{\\lambda(\\Omega)}{2\\pi}=1\\)\n\n\\(X(\\omega) = \\begin{cases} 0 & \\omega \\in \\Omega \\cap \\mathbb{Q}^c \\\\ 1 & \\omega \\in \\Omega \\cap \\mathbb{Q} \\end{cases}\\)\n\\(X^{-1}(B_1) = \\{w|X(w) \\in B_1 \\} = \\Omega \\cap \\mathbb{Q}^c \\rightarrow P(\\Omega \\cap \\mathbb{Q}^c)=1\\)\n\\(X^{-1}(B_2) = \\{w|X(w) \\in B_2 \\} = \\Omega \\cap \\mathbb{Q}\\rightarrow P(\\Omega \\cap \\mathbb{Q})=0\\)\n\\(\\nu(B) = \\begin{cases} 0 & 0 \\notin B \\\\ 1 & 0 \\in B \\end{cases}\\) 라고 정의하자.\nif \\(\\nu(B)=0 \\rightarrow 0 \\notin B \\rightarrow \\mu_X(B) = P \\circ X^{-1}(B) = 0\\)\n\\(\\therefore \\mu_X << \\nu\\)\n라돈니코딤 도함수를 정의하기 위해\n\\(\\forall B, \\nu(B)=0 \\rightarrow \\mu_X(B)=0\\)임을 보이자. (위에서 보임)\n\\(\\nu(B) \\neq 0\\)인 것은 생각하지 않아도 된다.\n\\(\\mu_X(B) = \\int_B f d\\nu\\)가 되려면\n만약 \\(\\nu(B) = 0 \\rightarrow \\int_B f d \\nu = 0 \\rightarrow \\mu_X(B) = 0 \\leftrightarrow \\mu_X << \\nu\\) 이므로\n=\\(\\int_B \\dfrac{d \\mu_X}{d\\nu} d\\nu= \\int_B \\dfrac{\\lambda(X^{-1}(B))}{2\\pi}dx\\)\n\\(f(X) = \\begin{cases} 1 & \\mathbb{Q} \\cap \\Omega \\\\ 0 & \\mathbb{Q}^c \\cap \\Omega \\end{cases}\\)\n\\(f=\\frac{d\\mu_X}{d\\nu}=\\frac{d}{d\\nu}\\int f d\\nu\\)\n\n\\(\\mu_X(B)=P(X^{-1}(B))=\\dfrac{\\lambda(X^{-1}(B))}{2\\pi}\\)이고\n라돈니코딤 도함수를 찾다.\n\\(\\int_B f d\\nu = \\int_B \\dfrac{d\\mu_X}{d\\nu} d\\nu = \\int_B \\dfrac{\\lambda(X^{-1}(dx)}{2\\pi}\\)\nApologies for the confusion. Let’s proceed to determine the appropriate σ-finite measure ν.\nGiven the function X defined as:\n\\[X(\\omega) = \\begin{cases} 0 & \\omega \\in \\Omega \\cap \\mathbb{Q}^c \\\\ 1 & \\omega \\in \\Omega \\cap \\mathbb{Q} \\end{cases}\\]\nwhere Ω is the probability space [0, 2π).\nTo find the measure ν, we need to consider the preimages of singletons {0} and {1} under the mapping X. Let’s denote these preimages as \\(X^(-1)({0}) and X^(-1)({1})\\), respectively.\n\\(X^(-1)({0})\\) consists of all ω in Ω for which \\(X(ω) = 0\\). Since \\(X(ω) = 0\\) for all \\(ω in Ω \\ [0, 2π), we have X^(-1)({0}) = Ω \\ [0, 2π).\\)\n\\(X^(-1)({1})\\) consists of all ω in Ω for which X(ω) = 1. Since \\(X(ω) = 1\\) for all \\(ω in [0, 2π) \\ Ω, we have X^(-1)({1}) = [0, 2π) \\ Ω.\\)\nNow, let’s define the measure ν on the Borel sets of [0, 2π) as follows:\nν(A) = λ(A) for A ∈ F, where F is the Borel σ-algebra on [0, 2π), and λ is the Lebesgue measure.\nTo specify ν completely, we need to determine its values on X^(-1)({0}) and X^(-1)({1}).\n\\(ν(X^{-1}({0})) = λ(\\frac{Ω}{[0, 2π)}) = λ(∅) = 0\\)\n\\(ν(X^{-1}({1})) = λ([0, 2π) \\ Ω) = λ([0, 2π)) - λ(Ω) = 2π - λ(Ω)\\)\nSince we are dealing with the probability space [0, 2π), we know that λ(Ω) = 2π, so ν(X^(-1)({1})) = 2π - 2π = 0.\nTherefore, we have:\nν(A) =\n\\[\\begin{cases} λ(A) & \\text{if } A \\neq X^{-1}({0}), X^{-1}({1}) \\\\ 0 & \\text{if } A = X^{-1}({0}), X^{-1}({1}) \\end{cases}\\]\nIn summary, the measure ν is the Lebesgue measure λ on [0, 2π), except for the sets X^(-1)({0}) and X^(-1)({1}), where ν assigns measure 0.\n<!!> To find a σ-finite measure ν such that μ_X << ν, we can define ν as the Lebesgue measure on the interval [0, 2π), denoted by λ. The Lebesgue measure is a σ-finite measure, and since our probability space is \\([0, 2π)\\) with the Borel σ-algebra, it is appropriate to choose λ as the measure ν.\nSo, we have: - ν(A) = λ(A) for all A ∈ F, where F is the Borel σ-algebra on [0, 2π). - μ_X is defined as the pushforward measure of P under the mapping X.\nTo determine the Radon-Nikodym derivative of μ_X with respect to ν, we need to compute the function f = dμ_X/dν.\nLet’s define the mapping \\(X: [0, 2π) → [0, 2π) as X(ω) = ω\\), which is the identity mapping. Since X is the identity mapping, we have \\(P(X^(-1)(A)) = P(A) for all A ∈ F\\).\nNow, we can compute the Radon-Nikodym derivative f as follows: \\(f(A) = dμ_X/dν(A) = P(X^(-1)(A)) / ν(A) = P(A) / ν(A) = (λ(A) / (2π)) / λ(A) = 1 / (2π) for all A ∈ F.\\)\nTherefore, the Radon-Nikodym derivative of μ_X with respect to \\(ν is f = 1 / (2π)\\), which is a constant function.\nThank you for the clarification. Based on the information you provided, it seems that \\(\\mu_X\\) is defined as the pushforward measure of \\(P\\) under a measurable function \\(X: \\Omega \\to \\mathbb{R}\\).\nLet’s define the measure \\(\\nu\\) as the Lebesgue measure on the interval \\([0,2\\pi)\\). In other words, for any measurable set \\(A \\subset [0,2\\pi)\\), we have \\(\\nu(A) = \\lambda(A)\\), where \\(\\lambda(A)\\) represents the Lebesgue measure of \\(A\\).\nSince \\(\\mu_X\\) is the pushforward measure of \\(P\\) under \\(X\\), we can define it as \\(\\mu_X(A) = P(X^{-1}(A))\\) for any measurable set \\(A \\subset \\mathbb{R}\\). Using the given probability measure \\(P(A) = \\frac{\\lambda(A)}{2\\pi}\\), we can write \\(\\mu_X(A) = \\frac{\\lambda(X^{-1}(A))}{2\\pi}\\).\nNow, the Radon-Nikodym derivative of \\(\\mu_X\\) with respect to \\(\\nu\\), denoted as \\(f = \\frac{d\\mu_X}{d\\nu}\\), represents a measurable function on \\(\\mathbb{R}\\) such that for any measurable set \\(A \\subset \\mathbb{R}\\), we have \\(\\mu_X(A) = \\int_A f d\\nu\\). In our case, we need to find this derivative explicitly.\nUsing the definition of \\(\\mu_X\\), we have:\n\\[\\int_A f d\\nu = \\int_A \\frac{d\\mu_X}{d\\nu} d\\nu = \\int_A \\frac{\\lambda(X^{-1}(dx))}{2\\pi}.\\]\nSince \\(\\nu\\) is the Lebesgue measure, \\(d\\nu = dx\\), and we can simplify the integral:\n\\[\\int_A f d\\nu = \\int_A \\frac{\\lambda(X^{-1}(dx))}{2\\pi} = \\int_A \\frac{\\lambda(X^{-1}(dx))}{2\\pi}.\\]\nTherefore, the Radon-Nikodym derivative \\(f\\) is given by:\n\\[f(x) = \\frac{d\\mu_X}{d\\nu}(x) = \\frac{\\lambda(X^{-1}(dx))}{2\\pi}.\\]\nPlease note that this expression holds for any measurable set \\(A \\subset \\mathbb{R}\\) and its corresponding preimage \\(X^{-1}(A)\\) under the function \\(X\\).\n설명해 주셔서 감사합니다. 제공한 정보에 따르면 \\(\\mu_X\\)는 측정 가능한 함수 \\(X: \\Omega \\to \\mathbb{R}\\)에서 \\(P\\)의 푸시포워드 측정으로 정의된 것으로 보입니다.\n측정 \\(\\nu\\)를 \\([0,2\\pi)\\) 간격에 대한 르베그 측정으로 정의합니다. 즉, 모든 측정 가능한 세트 \\(A \\digma [0,2\\pi]\\)에 대해 \\(\\nu(A) = \\lambda(A)\\)를 가지고 있으며, 여기서 \\(\\lambda(A)\\)는 \\(A\\)의 르베그 측정값을 나타냅니다.\n\\(\\mu_X\\)는 \\(X\\) 아래의 \\(P\\)의 푸시포워드 측정값이므로, 모든 측정 가능한 세트 \\(A \\mathbb{R}\\)에 대해 \\(\\mu_X(A) = P(X^{-1}(A)))\\)로 정의할 수 있습니다. 주어진 확률 측정 \\(P(A) = \\frac{\\lambda(A)}{2\\pi}\\)를 사용하여 \\(\\mu_X(A) = \\frac{\\lambda(X^{-1}(A)를 쓸 수 있습니다}{2\\pi}\\).\n이제 \\(f = \\frac{d\\mu_X}{d\\nu}\\)로 표시된 \\(\\nu\\)에 대한 \\(\\mu_X\\)의 라돈-니코딤 파생물은 \\(\\mathbb{에 대한 측정 가능한 함수를 나타냅니다R}\\) 측정 가능한 세트 \\(A \\mathbb{R}\\)에 대해 \\(\\mu_X(A) = \\int_A f d\\nu\\)를 가지고 있습니다. 우리의 경우, 우리는 이 파생물을 명시적으로 찾아야 합니다.\n\\(\\mu_X\\)의 정의를 사용하면 다음과 같은 이점이 있습니다:\n\\[\\int_A f d\\nu = \\int_A \\frac{d\\mu_X}{d\\nu} d\\nu = \\int_A \\frac{\\lambda(X^{-1}(dx)}{2\\pi}.\\]\n\\(\\nu\\)는 르베그 측도이므로 \\(d\\nu = dx\\) 적분을 단순화할 수 있습니다:\n\\[\\int_A f d\\nu = \\int_A \\frac{\\lambda(X^{-1}(dx))}{2\\pi} = \\int_A \\frac{\\lambda(X^{-1}(dx)}{2\\pi}.\\]\n따라서 라돈-니코딤 미분 \\(f\\)는 다음과 같이 주어집니다:\n\\[f(x) = \\frac{d\\mu_X}{d\\nu}(x) = \\frac{\\lambda(X^{-1}(dx)}{2\\pi}.\\]\n이 식은 측정 가능한 집합 \\(A \\subset \\mathbb{에 대해 고정됩니다R}\\) 및 함수 \\(X\\)에서 해당하는 사전 이미지 \\(X^{-1}(A)\\).\nThe function \\(X: \\Omega \\to \\mathbb{R}\\) you defined is an indicator function that distinguishes between rational and irrational numbers in the interval \\(\\Omega = [0, 2\\pi)\\). Specifically, \\(X(\\omega)\\) takes the value 0 if \\(\\omega\\) is irrational (\\(\\omega \\in \\Omega \\cap \\mathbb{Q}^c\\)), and it takes the value 1 if \\(\\omega\\) is rational (\\(\\omega \\in \\Omega \\cap \\mathbb{Q}\\)).\nTo find the Radon-Nikodym derivative \\(f = \\frac{d\\mu_X}{d\\nu}\\), we need to compute the measure \\(\\mu_X\\) and the measure \\(\\nu\\).\nGiven the probability measure \\(P\\) defined as \\(P(A) = \\frac{\\lambda(A)}{2\\pi}\\), we can calculate \\(\\mu_X(A)\\) for any measurable set \\(A \\subset \\mathbb{R}\\) using the definition of the pushforward measure:\n\\[\\mu_X(A) = P(X^{-1}(A)) = \\frac{\\lambda(X^{-1}(A))}{2\\pi}.\\]\nNow, since \\(X(\\omega)\\) only takes values 0 and 1, the preimage \\(X^{-1}(A)\\) can be written as the union of two disjoint sets: one corresponding to the values where \\(X(\\omega) = 0\\), and the other where \\(X(\\omega) = 1\\).\nLet’s consider \\(A_0 = A \\cap [0,2\\pi) \\cap \\mathbb{Q}^c\\) and \\(A_1 = A \\cap [0,2\\pi) \\cap \\mathbb{Q}\\), which represent the irrational and rational parts of \\(A\\), respectively. Then, we have:\n\\[X^{-1}(A) = X^{-1}(A_0) \\cup X^{-1}(A_1) = (\\Omega \\cap \\mathbb{Q}^c) \\cap A_0 \\cup (\\Omega \\cap \\mathbb{Q}) \\cap A_1.\\]\nNow, we can compute \\(\\mu_X(A)\\):\n\\[\\mu_X(A) = \\frac{\\lambda(X^{-1}(A))}{2\\pi} = \\frac{\\lambda((\\Omega \\cap \\mathbb{Q}^c) \\cap A_0 \\cup (\\Omega \\cap \\mathbb{Q}) \\cap A_1)}{2\\pi}.\\]\nSince \\(\\lambda\\) is the Lebesgue measure, which is countably additive and assigns measure 0 to single points, we have:\n\\[\\lambda((\\Omega \\cap \\mathbb{Q}^c) \\cap A_0) = \\lambda((\\Omega \\cap \\mathbb{Q}) \\cap A_1) = 0.\\]\nTherefore, we can simplify \\(\\mu_X(A)\\):\n\\[\\mu_X(A) = \\frac{\\lambda((\\Omega \\cap \\mathbb{Q}^c) \\cap A_0 \\cup (\\Omega \\cap \\mathbb{Q}) \\cap A_1)}{2\\pi} = \\frac{\\lambda(\\varnothing)}{2\\pi} = 0.\\]\nNow, let’s find the measure \\(\\nu\\), which is the Lebesgue measure on the interval \\([0,2\\pi)\\). For any measurable set \\(A \\subset [0,2\\pi)\\), we have \\(\\nu(A) = \\lambda(A)\\).\nFinally, we can compute the Radon-Nikodym derivative $f = $:\n\\[f(x) = \\frac{d\\mu_X}{d\\nu}(x) = \\frac{0}{\\lambda(x)} = 0.\\]\nTherefore, the Radon-Nikodym derivative of \\(\\mu_X\\) with respect to \\(\\nu\\) is 0.\n함수 \\(X: \\Omega \\to \\mathbb{정의한 R}\\)는 \\(\\Omega = [0, 2\\pi)\\) 간격에서 합리적인 숫자와 비합리적인 숫자를 구별하는 지표 함수입니다. 구체적으로 \\(X(\\omega)\\)는 \\(\\omega \\in \\Omega \\cap \\mathbb{인 경우 0 값을 사용합니다Q}^c\\)), \\(\\Omega\\)가 합리적인 경우(\\(\\Omega \\in \\Omega \\cap \\mathbb{Q}\\)) 값 1을 사용합니다.\n라돈-니코딤 미분 \\(f = \\frac{d\\mu_X}{d\\nu}\\)를 찾으려면 측정값 \\(\\mu_X\\)와 측정값 \\(\\nu\\)를 계산해야 합니다.\n\\(P(A) = \\frac{\\lambda(A)}{2\\pi}\\)로 정의된 확률 측정값 \\(P\\)를 고려할 때, 우리는 푸시포워드 측정값의 정의를 사용하여 측정 가능한 모든 세트 \\(A \\deline \\mathbb{R}\\)에 대해 \\(\\mu_X(A)\\)를 계산할 수 있습니다:\n\\[\\mu_X(A) = P(X^{-1}(A)) = \\frac{\\lambda(X^{-1}(A))}{2\\pi}.\\]\n이제 \\(X(\\omega)\\)는 값 0과 1만 사용하므로, 사전 이미지 \\(X^{-1}(A)\\)는 두 개의 분리된 집합의 결합으로 작성될 수 있습니다. 하나는 \\(X(\\omega) = 0\\)인 값에 해당하고 다른 하나는 \\(X(\\omega) = 1\\)인 값입니다.\n\\(A_0 = A \\cap [0,2\\pi] \\cap \\mathbb{를 생각해 보자Q}^c\\)와 \\(A_1 = A \\cap [0,2\\pi) \\cap \\mathbb{Q}\\)는 각각 \\(A\\)의 비합리적인 부분과 합리적인 부분을 나타냅니다. 그러면, 다음과 같은 이점이 있습니다:\n\\[X^{-1}(A) = X^{-1}(A_0) \\cup X^{-1}(A_1) = (\\Omega \\cap \\mathbb{Q}^c) \\cap A_0 \\cup (\\Omega \\cap \\mathbb{Q}) \\cap A_1.\\]\n이제 \\(\\mu_X(A)\\)를 계산할 수 있습니다:\n\\[\\mu_X(A) = \\frac{\\lambda(X^{-1}(A))}{2\\pi} = \\frac{\\lambda((\\Omega \\cap \\mathbb{Q}^c) \\cap A_0 \\cup(\\Omega \\cap \\mathbb{Q}) \\cap A_1)}{2\\pi}.\\]\n\\(\\lambda\\)는 르베그 측도이며 셀 수 없이 가법적이며 단일 지점에 측정값 0을 할당하기 때문에 다음과 같습니다:\n$\\(\\lambda((\\Omega \\cap \\mathbb{Q}^c) \\cap A_0) = \\lambda(((\\Omega \\cap \\mathbb{Q}) \\cap A_1) = 0.\\)\n따라서 \\(\\mu_X(A)\\)를 단순화할 수 있습니다:\n\\[\\mu_X(A) = \\frac{\\lambda((\\Omega \\cap \\mathbb{Q}^c) \\cap A_0 \\cup(\\Omega \\cap \\mathbb{Q})} {2\\pi} = \\frac{\\varnothing)} {2\\pi} = 0.\\]\n이제 간격 \\([0,2\\pi)\\)에 대한 르베그 측도인 측정 \\(\\nu\\)를 찾아보겠습니다. 모든 측정 가능한 세트 \\(A \\deta [0,2\\pi]\\)에 대해 \\(\\nu(A) = \\lambda(A)\\)가 있습니다.\n마지막으로 라돈-니코딤 미분 $f = $:\n\\[f(x) = \\frac{d\\mu_X}{d\\nu}(x) = \\frac{0}{\\frac(x)} = 0.\\]\n따라서 \\(\\nu\\)에 대한 \\(\\mu_X\\)의 라돈-니코딤 미분은 0입니다."
  },
  {
    "objectID": "posts/2023_06_13_fin.html#section-4",
    "href": "posts/2023_06_13_fin.html#section-4",
    "title": "15wk: 기말고사",
    "section": "(3)",
    "text": "(3)\n\\(X\\)의 평균을 구하라. 즉 \\(\\mathbb{E}(X)\\)를 계산하라.\n\\(E(X)=0 \\times (P \\circ X^{-1})(\\{0\\}) + 1 \\times (P \\circ X^{-1})(\\{1\\})\\)\n\\(f(X) = \\begin{cases} 1 & \\mathbb{Q} \\cap \\Omega :=A_1\\\\ 0 & \\mathbb{Q}^c \\cap \\Omega :=A_2 \\end{cases}\\)\n함수 \\(X:\\Omega \\to \\mathbb{R}\\)을 아래와 같이 정의할때\n\\(X(\\omega) = \\begin{cases} 0 & \\omega \\in \\Omega \\cap \\mathbb{Q}^c \\\\ 1 & \\omega \\in \\Omega \\cap \\mathbb{Q} \\end{cases}\\)\n\\(E(X) = \\int X dP = X(B_1) \\times P(\\{B_1\\}) + X(B_2) \\times P(\\{B_2\\})=0 \\times 1 + 1 \\times 0 = 0\\)\n\\(E(X) = \\int X dP = \\int_{\\Omega \\cap \\mathbb{Q}^c} X dP + \\int_{\\Omega \\cap \\mathbb{Q}} XdP= \\int_{W \\in \\Omega} X(w)dP(w)= 1x0 + 0x1 = 0\\)\n\\(X=0\\)인 경우,\n\\[\\int_{\\Omega \\cap \\mathbb{Q}^c} X \\, dP = \\int_{\\Omega \\cap \\mathbb{Q}^c} 0 \\, dP = 0\\]\n\\(X=1\\)인 경우,\n\\[\\int_{\\Omega \\cap \\mathbb{Q}} X \\, dP = \\int_{\\Omega \\cap \\mathbb{Q}} 1 \\, dP = \\int_{\\Omega \\cap \\mathbb{Q}} \\frac{\\lambda(A)}{2\\pi} \\, dA=\\frac{0}{2\\pi}=0\\]\n그러므로 기댓값은 0이다. \\[\\mathbb{E}(X) = \\int X \\, dP = \\int_{\\Omega \\cap \\mathbb{Q}^c} X \\, dP + \\int_{\\Omega \\cap \\mathbb{Q}} X \\, dP = 0 + 0 = 0\\]\nTo compute the average of the random variable \\(X\\) and find its expected value \\(\\mathbb{E}(X)\\), we can indeed use the expression:\n\\[\\mathbb{E}(X) = \\int X \\, dP\\]\nIn this case, \\(X\\) takes two possible values: \\(0\\) and \\(1\\). Let’s compute the integral for each value separately and then add them up.\nFor \\(X = 0\\), we need to integrate \\(X\\) over the set \\(\\Omega \\cap \\mathbb{Q}^c\\), which represents the subset of \\(\\Omega\\) where \\(\\omega\\) is irrational. Since \\(X\\) is always \\(0\\) for these values, the integral becomes:\n\\[\\int_{\\Omega \\cap \\mathbb{Q}^c} X \\, dP = \\int_{\\Omega \\cap \\mathbb{Q}^c} 0 \\, dP = 0\\]\nFor \\(X = 1\\), we need to integrate \\(X\\) over the set \\(\\Omega \\cap \\mathbb{Q}\\), which represents the subset of \\(\\Omega\\) where \\(\\omega\\) is rational. Since \\(X\\) is always \\(1\\) for these values, the integral becomes:\n\\[\\int_{\\Omega \\cap \\mathbb{Q}} X \\, dP = \\int_{\\Omega \\cap \\mathbb{Q}} 1 \\, dP\\]\nTo compute this integral, we can use the definition of \\(P(A)\\) for any \\(A \\in \\mathcal{F}\\):\n\\[\\int_{\\Omega \\cap \\mathbb{Q}} X \\, dP = \\int_{\\Omega \\cap \\mathbb{Q}} 1 \\, dP = \\int_{\\Omega \\cap \\mathbb{Q}} \\frac{\\lambda(A)}{2\\pi} \\, dA\\]\nSince \\(\\lambda(A)\\) represents the Lebesgue measure of set \\(A\\), and \\(\\Omega \\cap \\mathbb{Q}\\) is countable (since the rational numbers are countable), we have:\n\\[\\int_{\\Omega \\cap \\mathbb{Q}} X \\, dP = \\sum_{\\omega \\in \\Omega \\cap \\mathbb{Q}} \\frac{\\lambda(\\{\\omega\\})}{2\\pi}\\]\nNow, \\(\\lambda(\\{\\omega\\})\\) represents the measure of a singleton set, which is zero. Therefore, the sum above is also zero:\n\\[\\int_{\\Omega \\cap \\mathbb{Q}} X \\, dP = \\sum_{\\omega \\in \\Omega \\cap \\mathbb{Q}} \\frac{\\lambda(\\{\\omega\\})}{2\\pi} = 0\\]\nSince \\(X\\) is constant (\\(X=1\\)) on \\(\\Omega \\cap \\mathbb{Q}\\) and zero elsewhere, we can conclude that:\n\\[\\mathbb{E}(X) = \\int X \\, dP = \\int_{\\Omega \\cap \\mathbb{Q}^c} X \\, dP + \\int_{\\Omega \\cap \\mathbb{Q}} X \\, dP = 0 + 0 = 0\\]\nTherefore, the expected value of the random variable \\(X\\) is zero.\nTo find the average or expected value of the function \\(X\\), denoted as \\(\\mathbb{E}(X)\\), we need to consider the probabilities associated with the values that \\(X\\) takes.\nRecall that \\(X(\\omega)\\) takes the value 0 if \\(\\omega\\) is irrational and 1 if \\(\\omega\\) is rational. Since \\(\\Omega\\) is the interval \\([0, 2\\pi)\\), the set of rational numbers in \\(\\Omega\\) is dense, meaning that for any interval of positive length, it contains both rational and irrational numbers.\nTherefore, for any measurable set \\(A \\subset \\Omega\\), we have \\(P(X^{-1}(A)) = P(A \\cap \\mathbb{Q}^c) = \\frac{\\lambda(A \\cap \\mathbb{Q}^c)}{2\\pi}\\), where \\(\\lambda\\) denotes the Lebesgue measure.\nTo calculate \\(\\mathbb{E}(X)\\), we need to evaluate the expected value as a sum over the possible values of \\(X\\) weighted by their respective probabilities. In this case, we have:\n\\[\\mathbb{E}(X) = 0 \\cdot P(X = 0) + 1 \\cdot P(X = 1)\\]\nSince \\(X(\\omega)\\) only takes values 0 and 1, we can rewrite this as:\n\\[\\mathbb{E}(X) = P(X = 1)\\]\nNow, let’s calculate \\(P(X = 1)\\). Since \\(X = 1\\) when \\(\\omega\\) is rational, we can express this probability in terms of the Lebesgue measure:\n\\[P(X = 1) = \\frac{\\lambda(\\Omega \\cap \\mathbb{Q})}{2\\pi}\\]\nHowever, the set of rational numbers in the interval \\([0, 2\\pi)\\) has full Lebesgue measure since it is dense in the interval. Hence, \\(\\lambda(\\Omega \\cap \\mathbb{Q}) = \\lambda([0, 2\\pi)) = 2\\pi\\).\nTherefore, we have:\n\\[\\mathbb{E}(X) = P(X = 1) = \\frac{\\lambda(\\Omega \\cap \\mathbb{Q})}{2\\pi} = \\frac{2\\pi}{2\\pi} = 1\\]\nSo, the average or expected value of \\(X\\) is 1."
  },
  {
    "objectID": "posts/graph4-3.html",
    "href": "posts/graph4-3.html",
    "title": "CH4. 지도 그래프 학습(그래프 정규화 방법)",
    "section": "",
    "text": "그래프 머신러닝\ngithub"
  },
  {
    "objectID": "posts/graph4-3.html#load-dataset",
    "href": "posts/graph4-3.html#load-dataset",
    "title": "CH4. 지도 그래프 학습(그래프 정규화 방법)",
    "section": "Load Dataset",
    "text": "Load Dataset\n- 데이터셋: Cora\n\n7개의 클래스로 라벨링돼 있는 2,708개의 컴퓨터 사이언스 논문\n각 논문은 인용을 기반으로 다른 노드와 연결된 노드\n총 5,429개의 간선\n\n\nfrom stellargraph import datasets\n\n2023-04-06 21:44:50.486139: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\ndataset = datasets.Cora()\n\n\n%config Completer.use_jedi = False\n\n\ndataset.download()\n\n\nlabel_index = {\n      'Case_Based': 0,\n      'Genetic_Algorithms': 1,\n      'Neural_Networks': 2,\n      'Probabilistic_Methods': 3,\n      'Reinforcement_Learning': 4,\n      'Rule_Learning': 5,\n      'Theory': 6,\n  }\n\n\nG, labels = dataset.load()\n\n\nG: 네트워크 노드, 간선, BOW표현 설명\nlabea : 논문id와 클래스 중 하나 사이의 매핑\n훈련 샘플: 이웃과 관련된 정보가 포함 -> 훈련을 정규화 하는데 사용\n검증 샘플: 이웃과 관련된 정보 불포함 , 예측된 라벨은 노드 특증, bow표현에만 의존\n\n\nimport numpy as np\nfrom sklearn import preprocessing, feature_extraction, model_selection\n\n\nimport tensorflow as tf\nfrom tensorflow.train import Example, Features, Feature, Int64List, BytesList, FloatList\n\n\nGRAPH_PREFIX=\"NL_nbr\"\n\n\ndef _int64_feature(*value):\n    \"\"\"Returns int64 tf.train.Feature from a bool / enum / int / uint.\"\"\"\n    return Feature(int64_list=Int64List(value=list(value)))\n\ndef _bytes_feature(value):\n    \"\"\"Returns bytes tf.train.Feature from a string.\"\"\"\n    return Feature(\n        bytes_list=BytesList(value=[value.encode('utf-8')])\n    )\n\ndef _float_feature(*value):\n    return Feature(float_list=FloatList(value=list(value)))\n\n\n_int64_feature 함수는 bool, enum, int, uint 데이터 타입을 입력 받아 int64_list 타입의 tf.train.Feature 객체를 반환\n_bytes_feature 함수는 문자열 값을 입력 받아 utf-8로 인코딩하여 bytes_list 타입의 tf.train.Feature 객체를 반환\n_float_feature 함수는 float 데이터 타입을 입력 받아 float_list 타입의 tf.train.Feature 객체를 반환\n\n- 반지도 학습 데이터 셋 만드는 함수 정의\n\nfrom functools import reduce\nfrom typing import List, Tuple\nimport pandas as pd\nimport six\n\ndef addFeatures(x, y):\n    res = Features()\n    res.CopyFrom(x)\n    res.MergeFrom(y)\n    return res\n\ndef neighborFeatures(features: Features, weight: float, prefix: str):  # 객체, 가중치, 접두어 입력으로 받음\n    data = {f\"{prefix}_weight\": _float_feature(weight)}\n    for name, feature in six.iteritems(features.feature):\n        data[f\"{prefix}_{name}\"] = feature \n    return Features(feature=data)\n\ndef neighborsFeatures(neighbors: List[Tuple[Features, float]]):\n    return reduce(\n        addFeatures, \n        [neighborFeatures(sample, weight, f\"{GRAPH_PREFIX}_{ith}\") for ith, (sample, weight) in enumerate(neighbors)],\n        Features()\n    )\n\ndef getNeighbors(idx, adjMatrix, topn=5): #인덱스와 인접행렬 이용하여 이웃 데이터셋 추출 \n    weights = adjMatrix.loc[idx]\n    return weights[weights>0].sort_values(ascending=False).head(topn).to_dict()\n    \n\ndef semisupervisedDataset(G, labels, ratio=0.2, topn=5):  #라벨이 있는 데이터와 없는 데이터 추출\n     #ratio:라벨 유무 비율 설정\n     #topn: 함수에서 추출할 이웃 데이터셋 크기 설정\n    n = int(np.round(len(labels)*ratio)) \n    \n    labelled, unlabelled = model_selection.train_test_split(\n        labels, train_size=n, test_size=None, stratify=labels\n    )\n\n\n1. 노드 특징 df로 구성하고 그래프 인접행렬로 저장\n\nadjMatrix = pd.DataFrame.sparse.from_spmatrix(G.to_adjacency_matrix(), index=G.nodes(), columns=G.nodes())\n    \nfeatures = pd.DataFrame(G.node_features(), index=G.nodes())\n\n\n\n2. adjMatrix사용해 노드ID와 간선 가중치 반환하여 노드의 가장 가까운 TOPN이웃 검색하는 도우미 함수 구현\n\ndef getNeighbors(idx, adjMatrix, topn=5): #인덱스와 인접행렬 이용하여 이웃 데이터셋 추출 \n    weights = adjMatrix.loc[idx]\n    neighbors = weights[weights>0]\\\n        .sort_values(ascending=False)\\\n        .head(topn)\n    return [(k,v) for k, v in neighbors.iteritems()]\n    \n\n\n3. 정보를 단일 df로 병합\n\ndataset = {\n        index: Features(feature = {\n            #\"id\": _bytes_feature(str(index)), \n            \"id\": _int64_feature(index),\n            \"words\": _float_feature(*[float(x) for x in features.loc[index].values]), \n            \"label\": _int64_feature(label_index[label])\n        })\n        for index, label in pd.concat([labelled, unlabelled]).items()\n    }\n\nNameError: name 'labelled' is not defined\n\n\n\nfrom functools import reduce\nfrom typing import List, Tuple\nimport pandas as pd\nimport six\n\ndef addFeatures(x, y):\n    res = Features()\n    res.CopyFrom(x)\n    res.MergeFrom(y)\n    return res\n\ndef neighborFeatures(features: Features, weight: float, prefix: str):\n    data = {f\"{prefix}_weight\": _float_feature(weight)}\n    for name, feature in six.iteritems(features.feature):\n        data[f\"{prefix}_{name}\"] = feature \n    return Features(feature=data)\n\ndef neighborsFeatures(neighbors: List[Tuple[Features, float]]):\n    return reduce(\n        addFeatures, \n        [neighborFeatures(sample, weight, f\"{GRAPH_PREFIX}_{ith}\") for ith, (sample, weight) in enumerate(neighbors)],\n        Features()\n    )\n\ndef getNeighbors(idx, adjMatrix, topn=5):\n    weights = adjMatrix.loc[idx]\n    return weights[weights>0].sort_values(ascending=False).head(topn).to_dict()\n    \n\ndef semisupervisedDataset(G, labels, ratio=0.2, topn=5):\n    n = int(np.round(len(labels)*ratio))\n    \n    labelled, unlabelled = model_selection.train_test_split(\n        labels, train_size=n, test_size=None, stratify=labels\n    )\n    \n    adjMatrix = pd.DataFrame.sparse.from_spmatrix(G.to_adjacency_matrix(), index=G.nodes(), columns=G.nodes())\n    \n    features = pd.DataFrame(G.node_features(), index=G.nodes())\n    \n    dataset = {\n        index: Features(feature = {\n            #\"id\": _bytes_feature(str(index)), \n            \"id\": _int64_feature(index),\n            \"words\": _float_feature(*[float(x) for x in features.loc[index].values]), \n            \"label\": _int64_feature(label_index[label])\n        })\n        for index, label in pd.concat([labelled, unlabelled]).items()\n    }\n    \n    trainingSet = [\n        Example(features=addFeatures(\n            dataset[exampleId], \n            neighborsFeatures(\n                [(dataset[nodeId], weight) for nodeId, weight in getNeighbors(exampleId, adjMatrix, topn).items()]\n            )\n        ))\n        for exampleId in labelled.index\n    ]\n    \n    testSet = [Example(features=dataset[exampleId]) for exampleId in unlabelled.index]\n\n    serializer = lambda _list: [e.SerializeToString() for e in _list]\n    \n    return serializer(trainingSet), serializer(testSet)"
  },
  {
    "objectID": "posts/학회.html",
    "href": "posts/학회.html",
    "title": "sample",
    "section": "",
    "text": "일정:23. 6.29(목) ~ 7.1 (토)\n장소: 부경대학교(부산)\n발표신청 및 초록제출: 3.20.(월) ~ 4.20.(목)\n발표요약본제출(석사과정) : ~4.20.(목)\n포스터파일제출: ~ 5.19.(금)\n\n\n\n\n\n하계: 2023. 7.6.(목) ~ 7.7.(금)\n고려대학교\n발표신청 및 사전등록: 23.5.29.(월)\n초록 또는 논문제출: ~5.31.(수)\n\n\n\n\n\n일정: 6.23.(금) ~ 6.24.(토)\n장소: 강릉원주대학교\n발표논문 초록 제출: ~5.5.(금)\n발표 논문 제출: ~6.16.(금)\n학생논문: ~6.9.(금)"
  },
  {
    "objectID": "posts/ml with python 8.html",
    "href": "posts/ml with python 8.html",
    "title": "지도 학습",
    "section": "",
    "text": "ref\n\n선형대수와 통계학으로 배우는 머신러닝 with 파이썬\ngithub\n\n\n\nk-최근법 이웃 알고리즘\n\n\n# 데이터 불러오기\nfrom sklearn import datasets\nraw_iris = datasets.load_iris()\n\n# 피쳐/타겟\nX = raw_iris.data\ny = raw_iris.target\n\n# 트레이닝/테스트 데이터 분할\nfrom sklearn.model_selection import train_test_split\nX_tn, X_te, y_tn, y_te=train_test_split(X,y,random_state=0)\n\n#데이터 표준화\nfrom sklearn.preprocessing import StandardScaler\nstd_scale = StandardScaler()\nstd_scale.fit(X_tn)\nX_tn_std = std_scale.transform(X_tn)\nX_te_std  = std_scale.transform(X_te)\n\n\n# 학습\nfrom sklearn.neighbors import KNeighborsClassifier\nclf_knn =  KNeighborsClassifier(n_neighbors=2)\nclf_knn.fit(X_tn_std, y_tn)\n\nKNeighborsClassifier(n_neighbors=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier(n_neighbors=2)\n\n\n\n# 예측\nknn_pred = clf_knn.predict(X_te_std)\nprint(knn_pred)\n\n[2 1 0 2 0 2 0 1 1 1 1 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n 2]\n\n\n\n# 정확도\nfrom sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_te, knn_pred)\nprint(accuracy)\n\n0.9473684210526315\n\n\n\n# confusion matrix 확인 \nfrom sklearn.metrics import confusion_matrix\nconf_matrix = confusion_matrix(y_te, knn_pred)\nprint(conf_matrix)\n\n[[13  0  0]\n [ 0 15  1]\n [ 0  1  8]]\n\n\n\n# 분류 레포트 확인\nfrom sklearn.metrics import classification_report\nclass_report = classification_report(y_te, knn_pred)\nprint(class_report)\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        13\n           1       0.94      0.94      0.94        16\n           2       0.89      0.89      0.89         9\n\n    accuracy                           0.95        38\n   macro avg       0.94      0.94      0.94        38\nweighted avg       0.95      0.95      0.95        38\n\n\n\n\n\n선형 회귀 분석\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import LinearRegression \n\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n\n\n# 데이터 불러오기\nraw_boston = datasets.load_boston()\n\n# 피쳐, 타겟 데이터 지정\nX = raw_boston.data\ny = raw_boston.target\n\n# 트레이닝/테스트 데이터 분할\nX_tn, X_te, y_tn, y_te = train_test_split(X,y,random_state=1)\n\n\n# 데이터 표준화\nstd_scale = StandardScaler()\nstd_scale.fit(X_tn)\nX_tn_std = std_scale.transform(X_tn)\nX_te_std  = std_scale.transform(X_te)\n\n# 선형 회귀분석 학습\nclf_lr =  LinearRegression()\nclf_lr.fit(X_tn_std, y_tn)\n\n# 선형 회귀분석 모형 추정 계수 확인\nprint(clf_lr.coef_)\nprint(clf_lr.intercept_)\n\n# 릿지 회귀분석(L2 제약식 적용)\nclf_ridge = Ridge(alpha=1)\nclf_ridge.fit(X_tn_std, y_tn)\n\n# 릿지 회귀분석 모형 추정 계수 확인\nprint(clf_ridge.coef_)\nprint(clf_ridge.intercept_)\n\n# 라쏘 회귀분석(L1 제약식 적용)\nclf_lasso = Lasso(alpha=0.01)\nclf_lasso.fit(X_tn_std, y_tn)\n\n# 라쏘 회귀분석 모형 추정 계수 확인\nprint(clf_lasso.coef_)\nprint(clf_lasso.intercept_)\n\n# 엘라스틱넷\nclf_elastic = ElasticNet(alpha=0.01, l1_ratio=0.01)\nclf_elastic.fit(X_tn_std, y_tn)\n\n# 엘라스틱넷 모형 추정 계수 확인\nprint(clf_elastic.coef_)\nprint(clf_elastic.intercept_)\n\n# 예측\npred_lr = clf_lr.predict(X_te_std)\npred_ridge = clf_ridge.predict(X_te_std)\npred_lasso = clf_lasso.predict(X_te_std)\npred_elastic = clf_elastic.predict(X_te_std)\n\n# 모형 평가-R제곱값\nprint(r2_score(y_te, pred_lr))\nprint(r2_score(y_te, pred_ridge))\nprint(r2_score(y_te, pred_lasso))\nprint(r2_score(y_te, pred_elastic))\n\n# 모형 평가-MSE\nprint(mean_squared_error(y_te, pred_lr))\nprint(mean_squared_error(y_te, pred_ridge))\nprint(mean_squared_error(y_te, pred_lasso))\nprint(mean_squared_error(y_te, pred_elastic))\n- 회귀분석\n\\[\\hat w = (X^TX)^{-1}X^Ty\\]\n- 릿지 회귀 분석(L2제약식)\n\\[\\hat w^{ridge} = (X^TX+ \\lambda I_p)^{-1}X^Ty\\]\n\n\\(\\lambda\\) 계수의 사이즈 조절, 정규식의 크기 조절, 0에 가까울수록 최소 제곱 추정량에 가까워지며 무한대에 가까워질수록 릿지 해는 0에 가까워짐\n편향(bias)가 존재\n\n- 라쏘 회귀 분석(L1제약식)\n\\[\\hat w^{lasso}=argmin_w \\{(y-Xw)^T(y-Xw)+\\lambda(|w|-t) \\}\\]\n\n\n로지스틱 회귀 분석\n\n# 데이터 불러오기\nfrom sklearn import datasets\nraw_cancer = datasets.load_breast_cancer()\n\n# 피쳐, 타겟 데이터 지정\nX = raw_cancer.data\ny = raw_cancer.target\n\n# 트레이닝/테스트 데이터 분할\nfrom sklearn.model_selection import train_test_split\nX_tn, X_te, y_tn, y_te=train_test_split(X,y,random_state=0)\n\n\n\n\n#데이터 표준화\nfrom sklearn.preprocessing import StandardScaler\nstd_scale = StandardScaler()\nstd_scale.fit(X_tn)\nX_tn_std = std_scale.transform(X_tn)\nX_te_std  = std_scale.transform(X_te)\n\n\n# 로지스틱 회귀분석(L2 제약식 적용)\nfrom sklearn.linear_model import LogisticRegression\nclf_logi_l2 =  LogisticRegression(penalty='l2')\nclf_logi_l2.fit(X_tn_std, y_tn)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\n# 로지스틱 회귀분석 모형(L2 제약식 적용) 추정 계수\nprint(clf_logi_l2.coef_) # 추정 계수\nprint(clf_logi_l2.intercept_) # 상수항\n\n[[-0.29792942 -0.58056355 -0.3109406  -0.377129   -0.11984232  0.42855478\n  -0.71131106 -0.85371164 -0.46688191  0.11762548 -1.38262136  0.0899184\n  -0.94778563 -0.94686238  0.18575731  0.99305313  0.11090349 -0.3458275\n   0.20290919  0.80470317 -0.91626377 -0.91726667 -0.8159834  -0.86539197\n  -0.45539191  0.10347391 -0.83009341 -0.98445173 -0.5920036  -0.61086989]]\n[0.02713751]\n\n\n\n# 예측\npred_logistic = clf_logi_l2.predict(X_te_std)\nprint(pred_logistic)\n\n[0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1\n 0 1 0 0 1 0 1 1 0 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0\n 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1\n 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 1 0]\n\n\n\n# 확률값으로 예측\npred_proba = clf_logi_l2.predict_proba(X_te_std)\nprint(pred_proba)\n\n[[9.98638613e-01 1.36138656e-03]\n [3.95544804e-02 9.60445520e-01]\n [1.30896362e-03 9.98691036e-01]\n [1.24473354e-02 9.87552665e-01]\n [2.44132101e-04 9.99755868e-01]\n [4.50491513e-03 9.95495085e-01]\n [1.13985968e-04 9.99886014e-01]\n [1.82475894e-03 9.98175241e-01]\n [9.67965506e-05 9.99903203e-01]\n [1.75222878e-06 9.99998248e-01]\n [1.76572612e-01 8.23427388e-01]\n [8.24119135e-02 9.17588087e-01]\n [9.66067493e-06 9.99990339e-01]\n [5.39343196e-01 4.60656804e-01]\n [3.98187854e-01 6.01812146e-01]\n [9.95762760e-01 4.23724017e-03]\n [2.75612083e-03 9.97243879e-01]\n [9.99997097e-01 2.90271401e-06]\n [9.99926506e-01 7.34935682e-05]\n [9.99999997e-01 2.78313939e-09]\n [9.98738365e-01 1.26163489e-03]\n [9.81405399e-01 1.85946008e-02]\n [1.77902039e-02 9.82209796e-01]\n [9.65876713e-04 9.99034123e-01]\n [9.99464578e-01 5.35421808e-04]\n [6.73385015e-04 9.99326615e-01]\n [5.50833875e-05 9.99944917e-01]\n [9.69828919e-01 3.01710813e-02]\n [1.62119075e-03 9.98378809e-01]\n [9.99997821e-01 2.17867101e-06]\n [6.00571253e-05 9.99939943e-01]\n [9.99954808e-01 4.51921300e-05]\n [1.09252006e-01 8.90747994e-01]\n [9.97255978e-01 2.74402243e-03]\n [4.51047979e-06 9.99995490e-01]\n [9.97449456e-01 2.55054412e-03]\n [1.97830173e-02 9.80216983e-01]\n [9.99571529e-01 4.28470822e-04]\n [8.45566258e-03 9.91544337e-01]\n [9.99487912e-01 5.12087502e-04]\n [9.42409583e-01 5.75904174e-02]\n [8.34700429e-05 9.99916530e-01]\n [9.32505814e-01 6.74941855e-02]\n [8.11944408e-05 9.99918806e-01]\n [6.08911689e-02 9.39108831e-01]\n [9.99999999e-01 1.17373572e-09]\n [1.00967748e-06 9.99998990e-01]\n [1.48182234e-02 9.85181777e-01]\n [6.33630458e-04 9.99366370e-01]\n [9.99927519e-01 7.24813084e-05]\n [9.99989528e-01 1.04724511e-05]\n [8.04262948e-01 1.95737052e-01]\n [9.99965014e-01 3.49860375e-05]\n [1.36691079e-03 9.98633089e-01]\n [1.95330244e-03 9.98046698e-01]\n [5.74609838e-04 9.99425390e-01]\n [1.05063052e-03 9.98949369e-01]\n [7.96089471e-03 9.92039105e-01]\n [1.00288029e-02 9.89971197e-01]\n [9.99999999e-01 1.44073341e-09]\n [9.97609027e-01 2.39097260e-03]\n [9.99257870e-01 7.42129950e-04]\n [3.14309030e-05 9.99968569e-01]\n [4.40044150e-03 9.95599559e-01]\n [9.99897373e-01 1.02627439e-04]\n [1.52976144e-01 8.47023856e-01]\n [1.00000000e+00 2.39185116e-13]\n [9.99998777e-01 1.22317020e-06]\n [9.99999046e-01 9.53579837e-07]\n [7.96239235e-04 9.99203761e-01]\n [3.87033734e-01 6.12966266e-01]\n [9.99993469e-01 6.53125942e-06]\n [2.97085842e-03 9.97029142e-01]\n [8.09412134e-01 1.90587866e-01]\n [9.99996998e-01 3.00240009e-06]\n [1.75950117e-02 9.82404988e-01]\n [4.94325863e-05 9.99950567e-01]\n [3.51047770e-02 9.64895223e-01]\n [4.25841119e-04 9.99574159e-01]\n [2.09232609e-05 9.99979077e-01]\n [9.82374564e-01 1.76254356e-02]\n [1.00000000e+00 3.57855006e-10]\n [9.99988747e-01 1.12526453e-05]\n [5.94724730e-05 9.99940528e-01]\n [9.62731634e-01 3.72683662e-02]\n [1.69452548e-03 9.98305475e-01]\n [6.14966533e-05 9.99938503e-01]\n [6.36886875e-06 9.99993631e-01]\n [9.99902779e-01 9.72205364e-05]\n [1.00000000e+00 8.14423797e-11]\n [3.47458432e-05 9.99965254e-01]\n [5.53589378e-01 4.46410622e-01]\n [6.91462937e-01 3.08537063e-01]\n [9.99996851e-01 3.14924112e-06]\n [2.01951834e-03 9.97980482e-01]\n [2.39759190e-03 9.97602408e-01]\n [9.99999992e-01 7.92006333e-09]\n [1.03400237e-02 9.89659976e-01]\n [9.23218910e-03 9.90767811e-01]\n [9.80048490e-04 9.99019952e-01]\n [5.45753731e-09 9.99999995e-01]\n [3.09034901e-03 9.96909651e-01]\n [6.22819445e-03 9.93771806e-01]\n [1.49494565e-01 8.50505435e-01]\n [9.99994787e-01 5.21292981e-06]\n [6.02188244e-04 9.99397812e-01]\n [9.99995658e-01 4.34219020e-06]\n [9.49795077e-02 9.05020492e-01]\n [3.27428663e-01 6.72571337e-01]\n [1.72350019e-02 9.82764998e-01]\n [3.75686888e-02 9.62431311e-01]\n [9.99975711e-01 2.42887910e-05]\n [9.99911399e-01 8.86014791e-05]\n [8.65663331e-02 9.13433667e-01]\n [8.21398481e-04 9.99178602e-01]\n [2.45946373e-02 9.75405363e-01]\n [1.43898490e-01 8.56101510e-01]\n [1.58128486e-03 9.98418715e-01]\n [1.79682971e-02 9.82031703e-01]\n [1.18803803e-03 9.98811962e-01]\n [1.55728346e-02 9.84427165e-01]\n [1.43822197e-03 9.98561778e-01]\n [3.86829219e-01 6.13170781e-01]\n [2.65232841e-02 9.73476716e-01]\n [9.99999918e-01 8.17382381e-08]\n [1.28424726e-01 8.71575274e-01]\n [4.67709202e-01 5.32290798e-01]\n [2.58725940e-04 9.99741274e-01]\n [3.25269018e-05 9.99967473e-01]\n [4.00075207e-05 9.99959992e-01]\n [9.99901036e-01 9.89636008e-05]\n [1.27248974e-04 9.99872751e-01]\n [2.66411581e-04 9.99733588e-01]\n [2.13163719e-01 7.86836281e-01]\n [2.92511631e-02 9.70748837e-01]\n [2.37309476e-05 9.99976269e-01]\n [5.09465728e-01 4.90534272e-01]\n [6.17881971e-01 3.82118029e-01]\n [1.00000000e+00 1.46648090e-12]\n [8.41453252e-05 9.99915855e-01]\n [1.58701592e-03 9.98412984e-01]\n [1.26424968e-03 9.98735750e-01]\n [9.99999994e-01 5.81805301e-09]]\n\n\n\n# 정밀도\nfrom sklearn.metrics import precision_score\nprecision = precision_score(y_te, pred_logistic)\nprint(precision)\n\n0.9666666666666667\n\n\n\n# confusion matrix 확인 \nfrom sklearn.metrics import confusion_matrix\nconf_matrix = confusion_matrix(y_te, pred_logistic)\nprint(conf_matrix)\n\n[[50  3]\n [ 3 87]]\n\n\n\n# 분류 레포트 확인\nfrom sklearn.metrics import classification_report\nclass_report = classification_report(y_te, pred_logistic)\nprint(class_report)\n\n              precision    recall  f1-score   support\n\n           0       0.94      0.94      0.94        53\n           1       0.97      0.97      0.97        90\n\n    accuracy                           0.96       143\n   macro avg       0.96      0.96      0.96       143\nweighted avg       0.96      0.96      0.96       143\n\n\n\n\n\n나이브 베이즈(추후 다시)\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\n\n# 데이터 불러오기\nraw_wine = datasets.load_wine()\n\n# 피쳐, 타겟 데이터 지정\nX = raw_wine.data\ny = raw_wine.target\n\n# 트레이닝/테스트 데이터 분할\nX_tn, X_te, y_tn, y_te=train_test_split(X,y,random_state=0)\n\n# 데이터 표준화\nstd_scale = StandardScaler()\nstd_scale.fit(X_tn)\nX_tn_std = std_scale.transform(X_tn)\nX_te_std  = std_scale.transform(X_te)\n\n# 나이브 베이즈 학습\nclf_gnb = GaussianNB()\nclf_gnb.fit(X_tn_std, y_tn)\n\n# 예측\npred_gnb = clf_gnb.predict(X_te_std)\nprint(pred_gnb)\n\n# 리콜\nrecall = recall_score(y_te, pred_gnb, average='macro')\nprint(recall)\n\n# confusion matrix 확인 \nconf_matrix = confusion_matrix(y_te, pred_gnb)\nprint(conf_matrix)\n\n# 분류 레포트 확인\nclass_report = classification_report(y_te, pred_gnb)\nprint(class_report)\n\n\n의사결정나무(추후 다시)\n\n테스트 성능 평가는 엔트로피 이용\n엔트로피는 불순도(노드에 서로 다른 데이터가 얼마나 섞여 있는지) 정도를 측정하며 낮을수록 좋다.\n\n\\[Entropy(d) = - \\sum p(x) log P(x)\\]\n\\[= - \\sum_{i=1}^k p(i|d)log_2(p(i|d))\\]\n\n# 데이터 불러오기\nfrom sklearn import datasets\nraw_wine = datasets.load_wine()\n\n# 피쳐, 타겟 데이터 지정\nX = raw_wine.data\ny = raw_wine.target\n\n# 트레이닝/테스트 데이터 분할\nfrom sklearn.model_selection import train_test_split\nX_tn, X_te, y_tn, y_te=train_test_split(X,y,random_state=0)\n\n# 데이터 표준화\nfrom sklearn.preprocessing import StandardScaler\nstd_scale = StandardScaler()\nstd_scale.fit(X_tn)\nX_tn_std = std_scale.transform(X_tn)\nX_te_std  = std_scale.transform(X_te)\n\n\n# 의사결정나무 학습\nfrom sklearn import tree \nclf_tree = tree.DecisionTreeClassifier(random_state=0)\nclf_tree.fit(X_tn_std, y_tn)\n\n\nDecisionTreeClassifier(random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier(random_state=0)\n\n\n\n# 예측\npred_tree = clf_tree.predict(X_te_std)\nprint(pred_tree)\n\n\n[0 2 1 0 1 1 0 2 1 1 2 2 0 1 2 1 0 0 2 0 1 0 1 1 1 1 1 1 1 2 0 0 1 0 0 0 2\n 1 1 2 1 0 1 1 1]\n\n\n\n# f1 score\nfrom sklearn.metrics import f1_score\nf1 = f1_score(y_te, pred_tree, average='macro')\nprint(f1)\n\n0.9349141206870346\n\n\n\n# confusion matrix 확인 \nfrom sklearn.metrics import confusion_matrix\nconf_matrix = confusion_matrix(y_te, pred_tree)\nprint(conf_matrix)\n\n[[14  2  0]\n [ 0 20  1]\n [ 0  0  8]]\n\n\n\n# 분류 레포트 확인\nfrom sklearn.metrics import classification_report\nclass_report = classification_report(y_te, pred_tree)\nprint(class_report)\n\n              precision    recall  f1-score   support\n\n           0       1.00      0.88      0.93        16\n           1       0.91      0.95      0.93        21\n           2       0.89      1.00      0.94         8\n\n    accuracy                           0.93        45\n   macro avg       0.93      0.94      0.93        45\nweighted avg       0.94      0.93      0.93        45\n\n\n\n\n\n서포트 벡터 머신(추후 다시)\n\n\n크로스 밸리데이션(추후 다시)"
  },
  {
    "objectID": "posts/coin prediction.html",
    "href": "posts/coin prediction.html",
    "title": "미래 예측 데이터 분석(비트코인 시세 예측)",
    "section": "",
    "text": "python-data-analysis data\nData Source\n이것이 데이터 분석이다 with 파이썬\n\n%matplotlib inline\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")"
  },
  {
    "objectID": "posts/coin prediction.html#arima",
    "href": "posts/coin prediction.html#arima",
    "title": "미래 예측 데이터 분석(비트코인 시세 예측)",
    "section": "ARIMA",
    "text": "ARIMA\n\nAR:자기 자신의 과거를 정보로 사용. > 현재의 상태는 이전의 상태를 참고해서 계산\nMA: 이전 항에서의 오차를 이용해 현재 항의 상태 추론\nARMA: AR+MA\nARIMA: ARMA모델에 추세 변동 경향성 반영\n\n- 만약 ARIMA클래스 order=(2,1,2)이라면\n\n첫번째 2: AR이 몇번째 과거까지 바라보는지\n두번째 1: 차분(difference) - 현재 상태의 변수에서 바로 전 상태의 변수를 빼줌(경향성)\n세번째 2: MA가 몇번째 과거까지 바라보는지\n\n\nfrom statsmodels.tsa.arima.model import ARIMA\n\nimport statsmodels.api as sm"
  },
  {
    "objectID": "posts/coin prediction.html#학습",
    "href": "posts/coin prediction.html#학습",
    "title": "미래 예측 데이터 분석(비트코인 시세 예측)",
    "section": "학습",
    "text": "학습\n\n\nmodel = ARIMA(bitcoin_df.price.values, order=(2, 1, 2))\nmodel_fit = model.fit()\nprint(model_fit.summary())\n\n                               SARIMAX Results                                \n==============================================================================\nDep. Variable:                      y   No. Observations:                  365\nModel:                 ARIMA(2, 1, 2)   Log Likelihood               -2787.490\nDate:                Thu, 18 May 2023   AIC                           5584.980\nTime:                        14:55:27   BIC                           5604.466\nSample:                             0   HQIC                          5592.725\n                                - 365                                         \nCovariance Type:                  opg                                         \n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nar.L1          0.2418      0.305      0.794      0.427      -0.355       0.839\nar.L2          0.6072      0.200      3.038      0.002       0.215       0.999\nma.L1         -0.1513      0.306     -0.495      0.621      -0.751       0.448\nma.L2         -0.6705      0.205     -3.279      0.001      -1.071      -0.270\nsigma2      2.624e+05   1.05e+04     24.899      0.000    2.42e+05    2.83e+05\n===================================================================================\nLjung-Box (L1) (Q):                   0.07   Jarque-Bera (JB):               721.96\nProb(Q):                              0.79   Prob(JB):                         0.00\nHeteroskedasticity (H):               0.14   Skew:                             0.01\nProb(H) (two-sided):                  0.00   Kurtosis:                         9.90\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n\n\nfig = model_fit.plot_predict() # 학습 데이터에 대한 예측 결과입니다. (첫번째 그래프)\nresiduals = pd.DataFrame(model_fit.resid) # 잔차의 변동을 시각화합니다. (두번째 그래프)\nresiduals.plot()\nforecast_data = model_fit.forecast(steps=5) # 학습 데이터셋으로부터 5일 뒤를 예측합니다.\n\n# 테스트 데이터셋을 불러옵니다.\ntest_file_path = '../data/market-price-test.csv'\nbitcoin_test_df = pd.read_csv(test_file_path, names=['ds', 'y'])\n\npred_y = forecast_data[0].tolist() # 마지막 5일의 예측 데이터입니다. (2018-08-27 ~ 2018-08-31)\ntest_y = bitcoin_test_df.y.values # 실제 5일 가격 데이터입니다. (2018-08-27 ~ 2018-08-31)\npred_y_lower = [] # 마지막 5일의 예측 데이터의 최소값입니다.\npred_y_upper = [] # 마지막 5일의 예측 데이터의 최대값입니다.\nfor lower_upper in forecast_data[2]:\n    lower = lower_upper[0]\n    upper = lower_upper[1]\n    pred_y_lower.append(lower)\n    pred_y_upper.append(upper)\n\n\nplt.plot(pred_y, color=\"gold\") # 모델이 예상한 가격 그래프입니다.\nplt.plot(pred_y_lower, color=\"red\") # 모델이 예상한 최소가격 그래프입니다.\nplt.plot(pred_y_upper, color=\"blue\") # 모델이 예상한 최대가격 그래프입니다.\nplt.plot(test_y, color=\"green\") # 실제 가격 그래프입니다.\n\n\nplt.plot(pred_y, color=\"gold\") # 모델이 예상한 가격 그래프입니다.\nplt.plot(test_y, color=\"green\") # 실제 가격 그래프입니다.\n\n\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom math import sqrt\n\nrmse = sqrt(mean_squared_error(pred_y, test_y))\nprint(rmse)"
  },
  {
    "objectID": "posts/coin prediction.html#상한가-및-하한가-선정",
    "href": "posts/coin prediction.html#상한가-및-하한가-선정",
    "title": "미래 예측 데이터 분석(비트코인 시세 예측)",
    "section": "상한가 및 하한가 선정",
    "text": "상한가 및 하한가 선정\n\nbitcoin_df = pd.read_csv(file_path, names=['ds', 'y'])\n\n# 상한가를 설정합니다.\nbitcoin_df['cap'] = 20000\n\n# 상한가 적용을 위한 파라미터를 다음과 같이 설정합니다.\nprophet = Prophet(seasonality_mode='multiplicative', \n                  growth='logistic',\n                  yearly_seasonality=True,\n                  weekly_seasonality=True, daily_seasonality=True,\n                  changepoint_prior_scale=0.5)\nprophet.fit(bitcoin_df)\n\n\n\n# 5일을 내다보며 예측합니다.\nfuture_data = prophet.make_future_dataframe(periods=5, freq='d')\n\n# 상한가를 설정합니다.\nfuture_data['cap'] = 20000\nforecast_data = prophet.predict(future_data)\n\n\nfig = prophet.plot(forecast_data)\n\n\nbitcoin_test_df = pd.read_csv(test_file_path, names=['ds', 'y'])\n\n# 모델이 예상한 마지막 5일의 가격 데이터를 가져옵니다.\npred_y = forecast_data.yhat.values[-5:]\ntest_y = bitcoin_test_df.y.values\npred_y_lower = forecast_data.yhat_lower.values[-5:]\npred_y_upper = forecast_data.yhat_upper.values[-5:]\n\n\nplt.plot(pred_y, color=\"gold\") # 모델이 예상한 가격 그래프입니다.\nplt.plot(pred_y_lower, color=\"red\") # 모델이 예상한 최소가격 그래프입니다.\nplt.plot(pred_y_upper, color=\"blue\") # 모델이 예상한 최대가격 그래프입니다.\nplt.plot(test_y, color=\"green\") # 실제 가격 그래프입니다.\n\n\nplt.plot(pred_y, color=\"gold\") # 모델이 예상한 가격 그래프입니다.\nplt.plot(test_y, color=\"green\") # 실제 가격 그래프입니다.\n\n\nrmse = sqrt(mean_squared_error(pred_y, test_y))\nprint(rmse)"
  },
  {
    "objectID": "posts/coin prediction.html#이상치-제거",
    "href": "posts/coin prediction.html#이상치-제거",
    "title": "미래 예측 데이터 분석(비트코인 시세 예측)",
    "section": "이상치 제거",
    "text": "이상치 제거\n\n# 18000 이상의 데이터는 이상치라고 판단\nbitcoin_df = pd.read_csv(file_path, names=['ds', 'y'])\nbitcoin_df.loc[bitcoin_df['y'] > 18000, 'y'] = None\n\n\n# prophet 모델을 학습합니다.\nprophet = Prophet(seasonality_mode='multiplicative',\n                  yearly_seasonality=True,\n                  weekly_seasonality=True, daily_seasonality=True,\n                  changepoint_prior_scale=0.5)\nprophet.fit(bitcoin_df)\n\n# 5일단위의 미래를 예측합니다.\nfuture_data = prophet.make_future_dataframe(periods=5, freq='d')\nforecast_data = prophet.predict(future_data)\n\n# 예측 결과를 그래프로 출력합니다.\nfig = prophet.plot(forecast_data)\n\n\nbitcoin_test_df = pd.read_csv(test_file_path, names = ['ds', 'y'])\n\n# 모델이 예상한 마지막 5일의 가격 데이터를 가져옵니다.\npred_y = forecast_data.yhat.values[-5:]\ntest_y = bitcoin_test_df.y.values\npred_y_lower = forecast_data.yhat_lower.values[-5:]\npred_y_upper = forecast_data.yhat_upper.values[-5:]\n\n\nplt.plot(pred_y, color=\"gold\") # 모델이 예상한 가격 그래프입니다.\nplt.plot(pred_y_lower, color=\"red\") # 모델이 예상한 최소가격 그래프입니다.\nplt.plot(pred_y_upper, color=\"blue\") # 모델이 예상한 최대가격 그래프입니다.\nplt.plot(test_y, color=\"green\") # 실제 가격 그래프입니다.\n\n\nplt.plot(pred_y, color=\"gold\") # 모델이 예상한 가격 그래프입니다.\nplt.plot(test_y, color=\"green\") # 실제 가격 그래프입니다.\n\n\n# 테스트 데이터의 RMSE를 출력합니다.\nrmse = sqrt(mean_squared_error(pred_y, test_y))\nprint(rmse)"
  },
  {
    "objectID": "posts/coin prediction.html#arima-모델의-하이퍼-파라미터-선정하기",
    "href": "posts/coin prediction.html#arima-모델의-하이퍼-파라미터-선정하기",
    "title": "미래 예측 데이터 분석(비트코인 시세 예측)",
    "section": "ARIMA 모델의 하이퍼 파라미터 선정하기",
    "text": "ARIMA 모델의 하이퍼 파라미터 선정하기\n\nbitcoin_df = pd.read_csv(file_path, names = ['day', 'price'])\nbitcoin_df['day'] = pd.to_datetime(bitcoin_df['day'])\nbitcoin_df.index = bitcoin_df['day']\nbitcoin_df.set_index('day', inplace=True)\n\n\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nplot_acf(bitcoin_df.price.values)\nplot_pacf(bitcoin_df.price.values)\nplt.show()\n\n\nmodel = ARIMA(bitcoin_df.price.values, order=(15,2,0))\nmodel_fit = model.fit(trend='c',full_output=True, disp=True)\nprint(model_fit.summary())\n\n\nmodel = ARIMA(bitcoin_df.price.values, order=(2,1,2))\nmodel_fit = model.fit(trend='c',full_output=True, disp=True)\nprint(model_fit.summary())"
  },
  {
    "objectID": "posts/2023-06-13-fin.html",
    "href": "posts/2023-06-13-fin.html",
    "title": "15wk: 기말고사",
    "section": "",
    "text": "1. 다음을 읽고 참거짓을 판단하라. (30점)\n(1) 유리수집합 \\(\\mathbb{Q}\\)는 가산집합이며 유리수집합을 르벡메져로 측정하면 그 길이가 0이다. 즉 \\(\\lambda(\\mathbb{Q})=0\\) 이다.\nT\n(2) 르벡메져 \\(\\lambda\\)는 \\(\\mathbb{R}\\)의 모든 부분집합에 대하여 그 길이를 모순없이 정의가능하다. 즉 르벡메져는 \\((\\mathbb{R},2^{\\mathbb{R}})\\) 에서의 메져가 된다.\n\nF\n\n(3) 르벡메져 \\(\\lambda\\)는 임의의 \\(B \\in {\\cal R}\\)의 길이를 모순없이 정의가능하다. 즉 르벡메져는 \\((\\mathbb{R},{\\cal R})\\) 에서의 메져가 된다.\nF\n(4) 집합 \\(\\Omega\\)의 부분집합을 원소로 가지는 collection \\({\\cal F}\\)를 고려하자. 만약에 \\({\\cal F}\\)가 파이시스템이면서 동시에 람다시스템이라면 \\({\\cal F}\\)는 시그마필드이다.\nt\n(5) 아래와 같은 함수 \\(f\\)를 고려하자.\n\\[f(x) = \\begin{cases} 1 & x \\in \\mathbb{Q}\\\\  0 & x \\in \\mathbb{R}-\\mathbb{Q}\\end{cases}\\]\n위의 함수에 대한 르벡적분값은 무한대이다. 즉 \\(\\int_{\\mathbb{R}} f d\\lambda = \\infty\\) 이다.\n\nㅅ?\n\n(6) \\(X\\)가 가측공간 \\((\\Omega,{\\cal F})\\) 에서의 확률변수라는 의미는 모든 \\(B \\in {\\cal B}\\) 에 대하여 \\(\\{\\omega: X(\\omega) \\in B\\} \\in {\\cal F}\\) 를 만족한다는 의미이다.\nT\n(7) \\(X\\)가 가측공간 \\((\\Omega,{\\cal F})\\) 에서의 확률변수라면 \\(X\\)에 대응하는 분포(distribution) \\(\\mu_X\\)가 반드시 존재하며 \\(\\mu_X:=P \\circ X^{-1}\\)로 정의가능하다.\nT\n(8) \\(X\\)가 가측공간 \\((\\Omega,{\\cal F})\\) 에서의 확률변수이고, \\(X\\)에 대응하는 분포가 \\(\\mu_X\\)라고 하자. \\(\\mu_X\\)는 측도의 정의를 만족하지만 확률측도의 정의를 만족하지는 않는다.\nT\n(9) \\(X\\)가 가측공간 \\((\\Omega,{\\cal F})\\) 에서의 확률변수이고, \\(X\\)에 대응하는 분포가 \\(\\mu_X\\)라고 하자. \\(\\mu_X\\)에 대응하는 분포함수 \\(F_X(x) = \\mu_X((-\\infty,x])\\)는 항상 존재한다.\nT\n(10) \\(X\\)가 가측공간 \\((\\Omega,{\\cal F})\\) 에서의 확률변수이고 \\(F_X\\)가 \\(X\\)에 대응하는 분포함수라고 하자. 분포함수 \\(F_X\\)가 절대연속이라면 대응하는 \\(X\\)는 연속형확률변수이며 그 밀도함수 \\(f_X\\)가 존재한다.\nT\n\n\n2. 확률 (40점)\n(1) \\(\\Omega=\\{1,2,3,4\\}\\) 이라고 하고 \\({\\cal A} = \\{\\emptyset, \\{1\\},\\{2\\},\\{3,4\\},\\Omega\\}\\) 이라고 하자. 함수 \\(\\tilde{P}:{\\cal A} \\to [0,1]\\)를 아래와 같이 정의하자.\n\n\\(\\tilde{P}(\\emptyset) = 0\\)\n\\(\\tilde{P}(\\{1\\}) = 1/4\\)\n\\(\\tilde{P}(\\{2\\}) = 1/2\\)\n\\(\\tilde{P}(\\{3,4\\}) = 1/4\\)\n\\(\\tilde{P}(\\Omega) = 1\\)\n\n\\({\\cal A}\\)에서 \\(\\tilde{P}\\)와 일치하는 확률메져 \\(P\\)가 가측공간 \\((\\Omega,\\sigma({\\cal A}))\\) 에서 유일하게 존재하는가?\n(2) \\(\\Omega=\\{1,2,3,4\\}\\) 이라고 하고 \\({\\cal A} = \\{\\emptyset, \\{1,2\\},\\{2,3\\}, \\Omega\\}\\) 라고 하자. 함수 \\(\\tilde{P}:{\\cal A} \\to [0,1]\\)를 아래와 같이 정의하자.\n\n\\(\\tilde{P}(\\emptyset) = 0\\)\n\\(\\tilde{P}(\\{1,2\\}) = 1/2\\)\n\\(\\tilde{P}(\\{2,3\\}) = 1/2\\)\n\\(\\tilde{P}(\\Omega) = 1\\)\n\n\\({\\cal A}\\)에서 \\(\\tilde{P}\\)와 일치하는 확률메져 \\(P\\)가 가측공간 \\((\\Omega,\\sigma({\\cal A}))\\) 에서 유일하게 존재하는가?\n\n\n\n\n\n\n주의사항\n\n\n\n1. 위의 두 문제는 정답만 쓸 경우 답안으로 인정하지 않음. (1번답: 유일하게 존재함 <– 이러면 0점)\n2. 유일하게 존재하는 (혹은 존재하지 않는) 이유를 설명하고, 그러한 확률측도를 예시로 제시해야함.\n3. 즉 유일하게 존재하는 경우는 아래를 만족하는 \\(P\\)를 제시하고,\n\n\\(P\\) is probability measure on \\((\\Omega,\\sigma({\\cal A}))\\)\n\\(\\forall A \\in {\\cal A}: ~ P(A) = \\tilde{P}(A)\\)\n\n유일하게 존재하지 않는 경우는 아래를 만족하는 서로 다른 2개의 측도 \\(P_1,P_2\\)를 제시해야함.\n\n\\(P_1,P_2\\) are probability measures on \\((\\Omega,\\sigma({\\cal A}))\\)\n\\(\\forall A \\in {\\cal A}: ~ P_1(A) =P_2(A) = \\tilde{P}(A)\\)\n\\(\\exists B \\in \\sigma({\\cal A}):~ P_1(B) \\neq P_2(B)\\)\n\n\n\n\n\n3. 확률변수, 밀도함수, 기대값 (30점)\n아래와 같은 확률공간 \\((\\Omega,{\\cal F},P)\\)를 고려하라.\n\n\\(\\Omega=[0,2\\pi)\\)\n\\({\\cal F} = {\\cal R}\\cap [0,2\\pi) := \\{B\\cap [0,2\\pi): B \\in {\\cal R}\\}\\)\n\\(\\forall A \\in {\\cal F}:~ P(A)=\\frac{\\lambda (A)}{2\\pi}\\)\n\n\n\n\n\n\n\n이것이 확률공간인 이유 (문제로 낼려다가..)\n\n\n\n\\((\\Omega, {\\cal F}, P)\\)가 확률공간임을 보이기 위해서는 (1) \\({\\cal F}\\)가 \\(\\sigma\\)-field 이고 (2) \\(P\\)가 prob-msr on \\((\\Omega, {\\cal F})\\)임을 보이면 된다.\n(1) \\({\\cal F}\\) is \\(\\sigma\\)-filed of \\(\\Omega\\)\n\\({\\cal F}\\)는 \\(\\pi\\)-system 이고 \\(\\lambda\\)-system 이므로 시그마필드이다. (6주차 Dynkin’s \\(\\pi-\\lambda\\) theorem 증명을 위한 준비학습 참고)1\n(2) \\(P\\) is prob-msr on \\((\\Omega, {\\cal F})\\)\n\\(\\lambda\\)가 \\({\\cal F}\\)에서의 measure이므로, \\(P:=\\frac{1}{2\\pi}\\lambda\\) 역시 \\({\\cal F}\\)에서의 measure가 된다. 이때 \\(P\\)는 \\(P(\\Omega)=1\\)을 만족하므로 prob-msr가 된다.\n\n\n(1) 함수 \\(X:\\Omega \\to \\mathbb{R}\\)을 아래와 같이 정의할때\n\\[X(\\omega) = \\begin{cases} 0 & \\omega \\in \\Omega \\cap \\mathbb{Q}^c \\\\ 1 & \\omega \\in \\Omega \\cap \\mathbb{Q} \\end{cases}\\]\n\\(X\\)가 확률변수임을 보여라.\n(2) \\(\\mu_X << \\nu\\) 를 만족하는 \\(\\sigma\\)-finite measure \\(\\nu\\) 를 가측공간 \\((\\mathbb{R}, {\\cal R})\\)에서 정의하고, \\(\\mu_X\\)의 Radon-Nikodym derivative (w.r.t. \\(\\nu\\))\n\\[f:=\\frac{d\\mu_X}{d\\nu}\\]\n를 제시하라. 단, 여기에서 \\(\\mu_X : P \\circ X^{-1}\\) 이다.\n(3) \\(X\\)의 평균을 구하라. 즉 \\(\\mathbb{E}(X)\\)를 계산하라.\n\n\n\n\n\nFootnotes\n\n\n혹은 정석대로 시그마필드의 정의에 넣고 따져도 된다. 그런데 보통 그거보다 파이시스템과 람다시스템을 따지는게 더 편리하다.↩︎"
  },
  {
    "objectID": "posts/교수님.html",
    "href": "posts/교수님.html",
    "title": "sample",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport networkx as nx\nimport sklearn\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF \nfrom sklearn import metrics \n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x>0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부     \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\ndef build_graph_tripartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n                                                       df[\"cc_num\"].values.tolist() +\n                                                       df[\"merchant\"].values.tolist()))}\n    df[\"in_node\"]= df[\"cc_num\"].apply(lambda x: mapping[x])\n    df[\"out_node\"]=df[\"merchant\"].apply(lambda x:mapping[x])\n    \n        \n    G=nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\\\n                        [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")     \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")   \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")  \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n\n    return G\n    \n    \ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0]\n    df_minority = df[df.is_fraud==1]\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled]).reset_index(drop=True)\n    df_downsampled = df_downsampled.reindex(np.random.permutation(range(df_downsampled.shape[0]))).reset_index(drop=True)\n    return df_downsampled\n\ndef embedding(Graph):\n    # Graph -> X (feature)\n    _edgs = list(Graph.edges)\n    subGraph = Graph.edge_subgraph([_edgs[x] for x in range(len(Graph.edges))]).copy()\n    subGraph.add_nodes_from(list(set(Graph.nodes) - set(subGraph.nodes)))    \n    embedded = AverageEmbedder(Node2Vec(subGraph, weight_key='weight').fit(window=10).wv)\n    X = [embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in range(len(Graph.edges))]\n    # Graph -> y (label)\n    y = np.array(list(nx.get_edge_attributes(Graph, \"label\").values()))\n    return X,y \n\ndef anal(df):\n    Graph = build_graph_bipartite(df)\n    X,XX,y,yy = embedding(Graph)\n    lrnr = RandomForestClassifier(n_estimators=100, random_state=42) \n    lrnr.fit(X,y)\n    yyhat = lrnr.predict(XX)\n    df = pd.DataFrame({\n        'acc':[sklearn.metrics.accuracy_score(yy,yyhat)], \n        'pre':[sklearn.metrics.precision_score(yy,yyhat)], \n        'rec':[sklearn.metrics.recall_score(yy,yyhat)],\n        'f1':[sklearn.metrics.f1_score(yy,yyhat)]}\n    )    \n    return df\n\ndef our_sampling1(df):\n    cus_list = set(df.query('is_fraud==1').cc_num.tolist())\n    return df.query(\"cc_num in @ cus_list\")\n\n- 원본데이터\n\nfraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\nfraudTrain.shape\n\n(1048575, 22)\n\n\n\nfraudTrain.is_fraud.mean().round(5)\n\n0.00573\n\n\n\n총 1048575의 데이터 중 사기거래는 0.00573\n\n- 정상거래의 20퍼 랜덤샘플링\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\n\n\ndf02 = pd.concat([_df1,_df2]).reset_index(drop=True)\ndf02.shape\n\n(214520, 22)\n\n\n\ndf02.is_fraud.mean().round(5)\n\n0.028\n\n\n\n총 214,502의 데이터 중 사기거래는 0.028에 해당\n\n- downsample\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\ndf50.is_fraud.mean().round(5)\n\n0.5\n\n\n\n기존 파일에서 사기거래=1인 데이터의 숫자에 맞춰 downsampling함.\n12,012의 데이터중 0.5가 사기거래 비율\n\n\ndf02_tr,df02_test = sklearn.model_selection.train_test_split(df02)\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50)\n\n\nprint(df02_tr.shape)\nprint(df02_test.shape)\nprint(df50_tr.shape)\nprint(df50_test.shape)\n\n(160890, 22)\n(53630, 22)\n(9009, 22)\n(3003, 22)\n\n\n0.75/0.25로 나눴다.\n\n분석\n\nX=np.array(df50_tr.amt).reshape(-1,1)\nXX = np.array(df50_test.amt).reshape(-1,1)\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n\nlrnr = sklearn.linear_model.LogisticRegression()\n\n\nlrnr.fit(X,y)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\nyyhat = lrnr.predict(XX) \n\n\nyyhat\n\narray([1, 1, 1, ..., 0, 1, 0])\n\n\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n\n{m.__name__:m(yy,yyhat).round(6) for m in metrics}\n\n{'accuracy_score': 0.867799,\n 'precision_score': 0.952188,\n 'recall_score': 0.776603,\n 'f1_score': 0.855479}\n\n\n\nGtr = build_graph_tripartite(df50_tr)\nGtest = build_graph_tripartite(df50_test)\n\n\n    df=df50.copy();graph_type=nx.Graph()\n    mapping={x:node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n                                                       df[\"cc_num\"].values.tolist() +\n                                                       df[\"merchant\"].values.tolist()))}\n    df[\"in_node\"]= df[\"cc_num\"].apply(lambda x: mapping[x])\n    df[\"out_node\"]=df[\"merchant\"].apply(lambda x:mapping[x])\n    \n        \n    G=nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\\\n                        [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")\n     \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n    \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n    \n\n\n_G = nx.from_edgelist([[0,1],[0,2]])\n\n\nlen([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] + [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()])\n\n24024\n\n\n\nlen(df50)\n\n12012\n\n\n\ndf[(df.is_fraud==1)&(df.cc_num==3.598900e+15)]\n\n\n\n\n\n  \n    \n      \n      trans_date_trans_time\n      cc_num\n      merchant\n      category\n      amt\n      first\n      last\n      gender\n      street\n      city\n      ...\n      city_pop\n      job\n      dob\n      trans_num\n      unix_time\n      merch_lat\n      merch_long\n      is_fraud\n      in_node\n      out_node\n    \n  \n  \n    \n      686\n      2019-11-25 22:26\n      3.598900e+15\n      fraud_Kilback LLC\n      grocery_pos\n      320.26\n      Kenneth\n      Richards\n      M\n      994 Livingston Extensions Suite 918\n      Norwalk\n      ...\n      23805\n      Copy\n      1970-02-22\n      de391d925e1aa1f86cc77cf0e8f0e21b\n      1353882366\n      41.042223\n      -82.308471\n      1\n      12668\n      12614\n    \n    \n      925\n      2019-11-25 23:48\n      3.598900e+15\n      fraud_Kassulke PLC\n      shopping_net\n      948.05\n      Kenneth\n      Richards\n      M\n      994 Livingston Extensions Suite 918\n      Norwalk\n      ...\n      23805\n      Copy\n      1970-02-22\n      61df1730a076116730249af6e92c3ffb\n      1353887312\n      41.157798\n      -82.108063\n      1\n      12668\n      12241\n    \n    \n      1352\n      2019-11-25 3:38\n      3.598900e+15\n      fraud_Kuvalis Ltd\n      gas_transport\n      12.63\n      Kenneth\n      Richards\n      M\n      994 Livingston Extensions Suite 918\n      Norwalk\n      ...\n      23805\n      Copy\n      1970-02-22\n      4681eb64a8a2f95281afdcb7ddb43c4f\n      1353814696\n      41.082232\n      -82.864767\n      1\n      12668\n      12783\n    \n    \n      3113\n      2019-11-24 3:51\n      3.598900e+15\n      fraud_Koepp-Parker\n      grocery_pos\n      308.82\n      Kenneth\n      Richards\n      M\n      994 Livingston Extensions Suite 918\n      Norwalk\n      ...\n      23805\n      Copy\n      1970-02-22\n      726d491f80a2c4db905fefe75290cdc8\n      1353729072\n      40.523117\n      -82.073365\n      1\n      12668\n      13061\n    \n    \n      3219\n      2019-11-25 22:41\n      3.598900e+15\n      fraud_Bechtelar-Rippin\n      food_dining\n      127.81\n      Kenneth\n      Richards\n      M\n      994 Livingston Extensions Suite 918\n      Norwalk\n      ...\n      23805\n      Copy\n      1970-02-22\n      94f80338f56e539310f84c10c9c11fa8\n      1353883286\n      41.226756\n      -83.475536\n      1\n      12668\n      12392\n    \n    \n      7647\n      2019-11-25 22:44\n      3.598900e+15\n      fraud_Kihn, Abernathy and Douglas\n      shopping_net\n      968.41\n      Kenneth\n      Richards\n      M\n      994 Livingston Extensions Suite 918\n      Norwalk\n      ...\n      23805\n      Copy\n      1970-02-22\n      2a1a6053c916e39ed512338017bed155\n      1353883476\n      41.394215\n      -82.910453\n      1\n      12668\n      13072\n    \n    \n      9546\n      2019-11-25 22:21\n      3.598900e+15\n      fraud_Heathcote, Yost and Kertzmann\n      shopping_net\n      1074.33\n      Kenneth\n      Richards\n      M\n      994 Livingston Extensions Suite 918\n      Norwalk\n      ...\n      23805\n      Copy\n      1970-02-22\n      11ee1c8458d0d448d55de05c4c8a0fe9\n      1353882077\n      41.869650\n      -82.354703\n      1\n      12668\n      13303\n    \n    \n      11881\n      2019-11-24 23:35\n      3.598900e+15\n      fraud_Schaefer Ltd\n      kids_pets\n      16.53\n      Kenneth\n      Richards\n      M\n      994 Livingston Extensions Suite 918\n      Norwalk\n      ...\n      23805\n      Copy\n      1970-02-22\n      2a8b2e1497ff931108e47f5131ffbf5d\n      1353800139\n      41.431914\n      -82.661167\n      1\n      12668\n      12681\n    \n  \n\n8 rows × 24 columns\n\n\n\n\nimport matplotlib.pyplot as plt\n\n\nlen(df02.loc[(df02.cc_num==3.598900e+15),'amt'])\n\n95\n\n\n\nlen(df50.loc[(df02.cc_num==3.598900e+15),'amt'])\n\n3\n\n\n\nplt.plot(list(range(95)),df02.loc[(df02.cc_num==3.598900e+15),'amt'])\n\n\n\n\n\n_y= np.array(df02.loc[(df02.cc_num==3.598900e+15),'amt'])\n\n\ns=_y*0\n\n\nfor i in range(1,95):\n    s[i] = s[i-1] + _y[i]-np.mean(_y)\n\n\n_z=(_y-np.median(_y))/np.std(_y)\n\n\nplt.plot(_z)\n\n\n\n\n\n_t=(_y-np.mean(_y))/np.std(_y)\nplt.plot(_t)\n\n\n\n\n\nplt.plot(_z.cumsum())\n\n\n\n\n\nplt.plot(_t.cumsum())\n\n\n\n\n평균내면 -값 나와서.. 그래프가 이렇게\n\n_X,_y = embedding(Gtr)\n_XX,_yy = embedding(Gtest)\n\n\n\n\nGenerating walks (CPU: 1):   0%|          | 0/10 [00:00<?, ?it/s]\n\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:07<00:00,  1.39it/s]\n\n\n\n\n\nGenerating walks (CPU: 1):   0%|          | 0/10 [00:00<?, ?it/s]\n\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:07<00:00,  1.36it/s]\n\n\n\nprint(len(_X),len(_y), len(_XX),len(_yy))\n\n6006 6006 6006 6006\n\n\n\nprint(df50_tr.shape, len(_X), df50_test.shape, len(_XX))\n\n(9009, 22) 6006 (3003, 22) 6006\n\n\n\nX = np.array(df50_tr.amt).reshape(-1,1)\nXX = np.array(df50_test.amt).reshape(-1,1)\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n\nnp.stack(_X).shape\n\n(6006, 128)\n\n\n\nX.shape, np.stack(_X).shape\n\n((9009, 1), (6006, 128))\n\n\n_X는 그래프 임베딩 한건데, 뭘까\n\nlen(_yy),len(_y)\n\n(6006, 6006)\n\n\n\nlrnr = sklearn.linear_model.LogisticRegression()\n\n\nlrnr.fit(_X,_y)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\n_yyhat = lrnr.predict(_XX) \n\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n\n{m.__name__:m(_yy,_yyhat).round(6) for m in metrics}\n\n{'accuracy_score': 0.511655,\n 'precision_score': 0.577371,\n 'recall_score': 0.114673,\n 'f1_score': 0.191343}\n\n\n교수님것에서는 len(_X)= 2970 나왔는데\n\nG = build_graph_bipartite(df50)\n\n\ndf50.shape\n\n(12012, 22)\n\n\n\nX,XX,y,yy = embedding(G)\n\n\n\n\nGenerating walks (CPU: 1):   0%|          | 0/10 [00:00<?, ?it/s]\n\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04<00:00,  2.16it/s]\n\n\nValueError: not enough values to unpack (expected 4, got 2)\n\n\n\nnp.stack(X).shape, np.stack(XX).shape\n\n((9009, 1), (3003, 1))\n\n\n\nnp.array(y).shape,np.array(yy).shape\n\n((9009,), (3003,))\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nlrnr = RandomForestClassifier(n_estimators=100, random_state=42) \nlrnr.fit(np.stack(X),np.array(y))\n\nRandomForestClassifier(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(random_state=42)\n\n\n\nnp.mean(lrnr.predict(XX) == yy)\n\n0.8608058608058609\n\n\n\nanal(down_sample_textbook(df))\n\n\n\n\nGenerating walks (CPU: 1):   0%|          | 0/10 [00:00<?, ?it/s]\n\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04<00:00,  2.45it/s]\n\n\nValueError: not enough values to unpack (expected 4, got 2)\n\n\n\nanal(down_sample_textbook(our_sampling1(df)))"
  },
  {
    "objectID": "posts/graph8(frac.html",
    "href": "posts/graph8(frac.html",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(frac=0.3)",
    "section": "",
    "text": "ref\n\n그래프 머신러닝\ngithub\nCredit Card Transactions Fraud Detection Dataset\n컬리이미지\nnetworkx\n\n\n\n신용카드 거래에 대한 그래프 분석\n\n신용카드 거래 그래프 생성\n그래프에서 속성 및 커뮤니티 추출\n사기 거래 분류에 지도 및 비지도 머신러닝 알고리즘 적용\n\n\nimport pandas as pd\n\n\nimport os\nimport math\nimport numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndefault_edge_color = 'gray'\ndefault_node_color = '#407cc9'\nenhanced_node_color = '#f5b042'\nenhanced_edge_color = '#cc2f04'\n\n\nimport pandas as pd\ndf = pd.read_csv(\"~/Desktop/fraudTrain.csv\")\ndf = df[df[\"is_fraud\"]==0].sample(frac=0.20, random_state=42).append(df[df[\"is_fraud\"] == 1])\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      trans_date_trans_time\n      cc_num\n      merchant\n      category\n      amt\n      first\n      last\n      gender\n      street\n      ...\n      lat\n      long\n      city_pop\n      job\n      dob\n      trans_num\n      unix_time\n      merch_lat\n      merch_long\n      is_fraud\n    \n  \n  \n    \n      669418\n      669418\n      2019-10-12 18:21\n      4.089100e+18\n      fraud_Haley, Jewess and Bechtelar\n      shopping_pos\n      7.53\n      Debra\n      Stark\n      F\n      686 Linda Rest\n      ...\n      32.3836\n      -94.8653\n      24536\n      Multimedia programmer\n      1983-10-14\n      d313353fa30233e5fab5468e852d22fc\n      1350066071\n      32.202008\n      -94.371865\n      0\n    \n    \n      32567\n      32567\n      2019-01-20 13:06\n      4.247920e+12\n      fraud_Turner LLC\n      travel\n      3.79\n      Judith\n      Moss\n      F\n      46297 Benjamin Plains Suite 703\n      ...\n      39.5370\n      -83.4550\n      22305\n      Television floor manager\n      1939-03-09\n      88c65b4e1585934d578511e627fe3589\n      1327064760\n      39.156673\n      -82.930503\n      0\n    \n    \n      156587\n      156587\n      2019-03-24 18:09\n      4.026220e+12\n      fraud_Klein Group\n      entertainment\n      59.07\n      Debbie\n      Payne\n      F\n      204 Ashley Neck Apt. 169\n      ...\n      41.5224\n      -71.9934\n      4720\n      Broadcast presenter\n      1977-05-18\n      3bd9ede04b5c093143d5e5292940b670\n      1332612553\n      41.657152\n      -72.595751\n      0\n    \n    \n      1020243\n      1020243\n      2020-02-25 15:12\n      4.957920e+12\n      fraud_Monahan-Morar\n      personal_care\n      25.58\n      Alan\n      Parsons\n      M\n      0547 Russell Ford Suite 574\n      ...\n      39.6171\n      -102.4776\n      207\n      Network engineer\n      1955-12-04\n      19e16ee7a01d229e750359098365e321\n      1361805120\n      39.080346\n      -103.213452\n      0\n    \n    \n      116272\n      116272\n      2019-03-06 23:19\n      4.178100e+15\n      fraud_Kozey-Kuhlman\n      personal_care\n      84.96\n      Jill\n      Flores\n      F\n      639 Cruz Islands\n      ...\n      41.9488\n      -86.4913\n      3104\n      Horticulturist, commercial\n      1981-03-29\n      a0c8641ca1f5d6e243ed5a2246e66176\n      1331075954\n      42.502065\n      -86.732664\n      0\n    \n  \n\n5 rows × 23 columns\n\n\n\n\n_df = pd.read_csv(\"~/Desktop/fraudTrain.csv\")\n\n\ncus_list = set(_df.query('is_fraud==1').cc_num.tolist())\n_df2 = _df.query(\"cc_num in @ cus_list\")\n\n\n_df2.shape\n\n(651430, 23)\n\n\n\n_df2.groupby('is_fraud').agg({'category':np.sum})\n\n- 이분그래프\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x>0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G, {(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부 \n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\nG_bu = build_graph_bipartite(df, nx.Graph(name=\"Bipartite Undirect\"))\n\n\n지도학습\n\nfrom sklearn.utils import resample\n\ndf_majority = df[df.is_fraud==0]\ndf_minority = df[df.is_fraud==1]\n\ndf_maj_dowsampled = resample(df_majority,\n                             n_samples=len(df_minority),\n                             random_state=42)\n\ndf_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n\nprint(df_downsampled.is_fraud.value_counts())\nG_down = build_graph_bipartite(df_downsampled)\n\n1    6006\n0    6006\nName: is_fraud, dtype: int64\n\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_edges, test_edges, train_labels, test_labels = train_test_split(list(range(len(G_down.edges))), \n                                                                      list(nx.get_edge_attributes(G_down, \"label\").values()), \n                                                                      test_size=0.20, \n                                                                      random_state=42)\n\n\nedgs = list(G_down.edges)\ntrain_graph = G_down.edge_subgraph([edgs[x] for x in train_edges]).copy()\ntrain_graph.add_nodes_from(list(set(G_down.nodes) - set(train_graph.nodes)))\n\n\nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\nnode2vec_train = Node2Vec(train_graph, weight_key='weight')\nmodel_train = node2vec_train.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1):   0%|          | 0/10 [00:00<?, ?it/s]\n\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04<00:00,  2.48it/s]\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics \n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\nfor cl in classes:\n    embeddings_train = cl(keyed_vectors=model_train.wv) \n\n\ntrain_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\ntest_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n\n\nrf = RandomForestClassifier(n_estimators=1000, random_state=42) \n\n\nrf.fit(train_embeddings, train_labels)\n\nRandomForestClassifier(n_estimators=1000, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(n_estimators=1000, random_state=42)\n\n\n\n    #\n    y_hat = rf.predict_proba(test_embeddings)\n    y_pred = np.argmax(y_hat,axis=1)\n    #y_pred = rf.predict(test_embeddings)\n\n\n\n\n    print(cl)\n    print('Precision:', metrics.precision_score(test_labels, y_pred)) \n    print('Recall:', metrics.recall_score(test_labels, y_pred)) \n    print('F1-Score:', metrics.f1_score(test_labels, y_pred)) \n\n<class 'node2vec.edges.WeightedL2Embedder'>\nPrecision: 0.6481481481481481\nRecall: 0.028641571194762683\nF1-Score: 0.054858934169278985"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "sample",
    "section": "",
    "text": "imports\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n2023년 학회 발표 준비\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n분석\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n15wk: 기말고사\n\n\n\n\n\n\n\n\n\n\n\n\nJun 13, 2023\n\n\n김보람\n\n\n\n\n\n\n\n\n15wk: 기말고사\n\n\n\n\n\n\n\n\n\n\n\n\nJun 13, 2023\n\n\n최규빈\n\n\n\n\n\n\n\n\n미래 예측 데이터 분석(비트코인 시세 예측)\n\n\n\n\n\n\n\n회귀 분석\n\n\n\n\n\n\n\n\n\n\n\nMay 18, 2023\n\n\n김보람\n\n\n\n\n\n\n\n\n지도 학습\n\n\n\n\n\n\n\n지도 학습\n\n\n\n\n\n\n\n\n\n\n\nMay 11, 2023\n\n\n김보람\n\n\n\n\n\n\n\n\nCH4. 지도 그래프 학습(그래프 정규화 방법)\n\n\n\n\n\n\n\ngraph\n\n\n\n\n\n\n\n\n\n\n\nApr 6, 2023\n\n\n김보람\n\n\n\n\n\n\n\n\nCH8. 신용카드 거래에 대한 그래프 분석(frac=0.3)\n\n\n\n\n\n\n\ngraph\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2023\n\n\n김보람\n\n\n\n\n\n\n\n\nVAE(-ing)\n\n\n\n\n\n\n\nVAE\n\n\nAuto-Encoding Variational Bayes\n\n\n\n\n\n\n\n\n\n\n\nMar 14, 2023\n\n\n김보람\n\n\n\n\n\n\nNo matching items"
  }
]